[{"authors":["admin"],"categories":null,"content":"I am a researcher at Innovation Labs, Tata Consultancy Services. My research interests lie in the intersection of Computer Vision, Computer Graphics and Deep Learning with an application domain of Augmented and Virtual Reality. I completed my undergrad with Computer Science and Engineering major at Indraprastha Institute of Information Technology Delhi (IIIT-Delhi).\nLonger version of my resume is available here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://srihegde.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a researcher at Innovation Labs, Tata Consultancy Services. My research interests lie in the intersection of Computer Vision, Computer Graphics and Deep Learning with an application domain of Augmented and Virtual Reality. I completed my undergrad with Computer Science and Engineering major at Indraprastha Institute of Information Technology Delhi (IIIT-Delhi).\nLonger version of my resume is available here.","tags":null,"title":"Srinidhi Hegde","type":"authors"},{"authors":["S. Hegde","R. Prasad","R. Hebbalaguppe","V. Kumar"],"categories":null,"content":"","date":1588530600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588530600,"objectID":"10846a107312909bfeea26f6a647a5e6","permalink":"https://srihegde.github.io/publication/icassp2020/","publishdate":"2020-05-04T00:00:00+05:30","relpermalink":"/publication/icassp2020/","section":"publication","summary":"The holy grail in deep neural network research is porting the memory- and computation-intensive network models on embedded platforms with a minimal compromise in model accuracy. To this end, we propose a novel approach, termed as Variational Student, where we reap the benefits of compressibility of the knowledge distillation (KD) framework, and sparsity inducing abilities of variational inference (VI) techniques. Essentially, we build a sparse student network, whose sparsity is induced by the variational parameters found via optimizing a loss function based on VI, leveraging the knowledge learnt by an accurate but complex pre-trained teacher network. Further, for sparsity enhancement, we also employ a Block Sparse Regularizer on a concatenated tensor of teacher and student network weights. We demonstrate that the marriage of KD and the VI techniques inherits compression properties from the KD framework, and enhances levels of sparsity from the VI approach, with minimal compromise in the model accuracy. We benchmark our results on LeNet MLP and VGGNet (CNN) and illustrate a memory footprint reduction of 64x and 213x on these MLP and CNN variants, respectively, without a need to retrain the teacher network. Furthermore, in the low data regime, we observed that our method outperforms state-of-the-art Bayesian techniques in terms of accuracy.","tags":[],"title":"Variational Student: Learning Compact and Sparser Networks in Knowledge Distillation Framework","type":"publication"},{"authors":["S. Hegde","J. Maurya","A. Kalkar","R. Hebbalaguppe"],"categories":null,"content":"","date":1583001000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583001000,"objectID":"33d131c67ca378e17eb6fe1371966a3e","permalink":"https://srihegde.github.io/publication/wacv2020/","publishdate":"2020-03-01T00:00:00+05:30","relpermalink":"/publication/wacv2020/","section":"publication","summary":"","tags":[],"title":"SmartOverlays: A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces","type":"publication"},{"authors":["S. Yalburgi","T. Dash","R. Hebbalaguppe","S. Hegde","A. Srinivasan"],"categories":null,"content":"","date":1577817000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577817000,"objectID":"08737ab22db38567412fd45a2fdd9974","permalink":"https://srihegde.github.io/publication/esann2020/","publishdate":"2020-01-01T00:00:00+05:30","relpermalink":"/publication/esann2020/","section":"publication","summary":"In this paper we introduce Iterative Knowledge Distillation (IKD), the process of successively minimizing models based on the Knowledge Distillation (KD) approach in Hinton et al. We study two variations of IKD, called parental- and ancestral- training. Both use a single-teacher, and result in a single-student model: the differences arise from which model is used as a teacher. Our results provide support for the utility of the IKD procedure, in the form of increased model compression, without significant losses in predictive accuracy. An important task in IKD is choosing the right model(s) to act as a teacher for a subsequent iteration. Across the variations of IKD studied, our results suggest that the most recent model constructed (parental-training) is the best single teacher for the model in the next iteration. This result suggests that training in IKD can proceed without requiring us to keep all models in the sequence.","tags":[],"title":"An Empirical Study of Iterative Knowledge Distillation for Neural Network Compression","type":"publication"},{"authors":[],"categories":[],"content":"Compressing the memory-intensive DNN models with a minimal compromise in model accuracy using variational methods and knowledge distillation. Another part of projects involves proposing theoretical guarantees on the knowledge distillation models for efficient neural architecture search.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"831d1ad0dbf77e63346af26bafe8891e","permalink":"https://srihegde.github.io/project/nncomp/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/nncomp/","section":"project","summary":"Compressing deep neural network models for memory constrained devices","tags":["neural networks","deep learning","optimization","variational inference"],"title":"Deep Neural Network Compression","type":"project"},{"authors":[],"categories":[],"content":"Automating the animation through learning and transferring motion cues from the real RGB-D videos to virtual 3D meshes. We use graph based structure correspondences to map the motion between the two 3D entities such as point cloud and 3D meshes.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"40bbe7991dfce3ba482c89f0ca6e1083","permalink":"https://srihegde.github.io/project/animation/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/animation/","section":"project","summary":"Automating animation through motion transfer from real scenes.","tags":["computer graphics","computer vision","animation","machine learning"],"title":"Reverse VooDoo","type":"project"},{"authors":[],"categories":[],"content":"A novel method for the placement of labels corresponding to objects of interest in images/videos/live feeds that is non-intrusive, relevant and temporally coherent. We employ different techniques ranging from search space optimization to visual saliency based neural network framework.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"2cd5acc906258452e0e9aa258e8d59ba","permalink":"https://srihegde.github.io/project/smartov/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/project/smartov/","section":"project","summary":"A non-intrusive, relevant and temporally coherent overlay placement technique","tags":["situated visualization","augmented reality","computer vision","visual saliency"],"title":"Non Intrusive Overlay Placement in User Interfaces","type":"project"},{"authors":null,"categories":null,"content":"","date":1547420400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547420400,"objectID":"2537882a5aeadfef95adfe3a74f3cedd","permalink":"https://srihegde.github.io/news/14-01-2019/","publishdate":"2019-01-14T00:00:00+01:00","relpermalink":"/news/14-01-2019/","section":"news","summary":"","tags":null,"title":"Selected as mentor for PanIIT Hackathon at [TCS-PanIIT Conclave 2019](https://www.tcs.com/paniit-conclave-2019) organized by TCS and IIT Delhi.","type":"news"},{"authors":["G. Garg","S. Hegde","R. Perla","V. Jain","L. Vig","R. Hebbalaguppe"],"categories":null,"content":"","date":1538332200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538332200,"objectID":"15188d8fad6a9fe827f1771bd7ec8392","permalink":"https://srihegde.github.io/publication/eccv2018/","publishdate":"2018-10-01T00:00:00+05:30","relpermalink":"/publication/eccv2018/","section":"publication","summary":"Hand gestures form a natural way of interaction on Head-Mounted Devices (HMDs) and smartphones. HMDs such as the Microsoft HoloLens and ARCore/ARKit platform enabled smartphones are expensive and are equipped with powerful processors and sensors such as multiple cameras, depth and IR sensors to process hand gestures. To enable mass market reach via inexpensive Augmented Reality (AR) headsets without built-in depth or IR sensors, we propose a real-time, in-air gestural framework that works on monocular RGB input, termed, DrawInAir.DrawInAir uses fingertip for writing in air analogous to a pen on paper. The major challenge in training egocentric gesture recognition models is in obtaining sufficient labeled data for end-to-end learning. Thus, we design a cascade of networks, consisting of a CNN with differentiable spatial to numerical transform (DSNT) layer, for fingertip regression, followed by a Bidirectional Long Short-Term Memory(Bi-LSTM), for a real-time pointing hand gesture classification. We highlight how a model,that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better overend-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model. We showthat the framework takes 1.73s to run end-to-end and has a low memory footprint of 14MB while achieving an accuracy of 88.0% on egocentric video dataset.","tags":[],"title":"DrawInAir: A Lightweight Gestural Interface Based on Fingertip Regression","type":"publication"},{"authors":["N. Rakholia","S. Hegde","R. Hebbalaguppe"],"categories":null,"content":"","date":1538332200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538332200,"objectID":"4dc7d3c2966a164272530172b993be87","permalink":"https://srihegde.github.io/publication/icip2018/","publishdate":"2018-10-01T00:00:00+05:30","relpermalink":"/publication/icip2018/","section":"publication","summary":"Textual overlays/labels add contextual information in Augmented Reality (AR) applications. The spatial placement of labels is a challenging task due to constraints that labels (i) are not occluding the object/scene of interest, and, (ii) are optimally placed for better interpretation of scene. To this end, we present a novel method for optimal placement of labels for AR. We formulate this method by an objective function that minimizes both occlusion with visually salient regions in scenes of interest, and the temporal jitter for facilitating coherence in real-time AR applications. The main focus of proposed algorithm is real-time label placement on low-end android phones/tablets. The sophisticated state-of-the-art algorithms for optimal positioning of textual label work only on the images and are often inefficient for real-time performance on those devices. We demonstrate the efficiency of our method by porting the algorithm on a smart-phone/tablet. Further, we capture objective and subjective metrics to determine the efficacy of the method; objective metrics include computation time taken for determining the label location and Label Occlusion over Saliency (LOS) score over salient regions in the scene. Subjective metrics include position, temporal coherence in the overlay, color and responsiveness.","tags":[],"title":"Where To Place: A Real-Time Visual Saliency Based Label Placement for Augmented Reality Applications","type":"publication"},{"authors":[],"categories":[],"content":"Hand gesture classification through fingertip coordinate regression in a temporal model for touch-less interactions in AR. We highlight how a model, that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better over end-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model.\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"95083189b28bb1a09399d31cc3c5e338","permalink":"https://srihegde.github.io/project/drawinair/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/project/drawinair/","section":"project","summary":"A hand gesture based in-air touch-less interaction technique for AR applications","tags":["hand gestures","computer vision","augmented reality","deep learning"],"title":"Draw In Air","type":"project"},{"authors":[],"categories":[],"content":"Employing CNNs for an end-to-end reconstruction of the indoor scenes through camera relocalization, through PoseNet, and depth estimation, through multi-scale fully convolutional network, from a single RGB image during inference and registering the 3D reconstructed patches through iterative closest point algorithm. A portion of the dataset collected during the project is also released.\n","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"f109aed8fe02906bc2c731735cb883dc","permalink":"https://srihegde.github.io/project/btp/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/project/btp/","section":"project","summary":"Developing an end to end pipeline for automatic generation of 3D models of indoor scenes.","tags":["deep learning","3D reconstruction","camera relocalization","computer vision"],"title":"Robust 3D Reconstruction of Indoor Scenes using Deep Learning","type":"project"},{"authors":["S. Hegde","R. Perla","R. Hebbalaguppe","E. Hassan"],"categories":null,"content":"","date":1464719400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464719400,"objectID":"03671fcde735b5d8d799d0ebf19df59e","permalink":"https://srihegde.github.io/publication/ismar2016/","publishdate":"2016-06-01T00:00:00+05:30","relpermalink":"/publication/ismar2016/","section":"publication","summary":"The existing, sophisticated AR gadgets in the market today are mostly exorbitantly priced. This limits their usage for the upcoming academic research institutes and also their reach to the mass market in general. Among the most popular and frugal head mounts, Google Cardboard (GC) and Wearality are video-see-through devices that can provide immersible AR and VR experiences with a smartphone. Stereo-rendering of camera feed and overlaid information on smartphone helps us experience AR with GC. These frugal devices have limited user-input capability, allowing user interactions with GC such as head tilting, magnetic trigger and conductive lever. Our paper proposes a reliable and intuitive gesture based interaction technique for these frugal devices. The hand gesture recognition employs the Gaussian Mixture Models (GMM)based on human skin pixels and tracks segmented foreground using optical flow to detect hand swipe direction for triggering a relevant event. Real-time performance is achieved by implementing the hand gesture recognition module on a smartphone and thus reducing the latency. We augment real-time hand gestures as new GC’s interface with its evaluation done in terms of subjective metrics and with the available user interactions in GC.","tags":[],"title":"GestAR: Real Time Gesture Interaction for AR with Egocentric View","type":"publication"},{"authors":[],"categories":[],"content":"This work develops a distributed fault tolerant area coverage algorithm, resulting in quick detection of the faulty agent under limited communication constraints and redistributes the area without conflicts.\n","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"e500c976dc3bf8d2d7047c9d11fe6935","permalink":"https://srihegde.github.io/project/mas/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/project/mas/","section":"project","summary":"Analysed Multi-Agent area coverage problem for surveillance purposes.","tags":["multi-agent systems","area coverage","scheduling","robotics"],"title":"Distributed Fault Tolerant Multi-Robot Area Coverage Under Limited Communication Ranges","type":"project"},{"authors":[],"categories":[],"content":"Realistic models of vegetations are very essential piece of immersive virtual environment simulations. We develop a novel technique to convert a single captured image of a vegetation to a 3D model using L-Systems, a context-free grammar that we adapt to procedurally model vegetation. We also propose a pipeline that is semi-automated with manual interventions for accurate identification of tree branches and trunk.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7ba1e13a3b867f0d96e2ccdc01f65e5d","permalink":"https://srihegde.github.io/project/lsys/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/lsys/","section":"project","summary":"Procedural modelling of vegetation using context-free grammar","tags":["Computer Graphics","3D Modelling","Context-free Grammar"],"title":"Modelling Vegetation with L-Systems Using an Image","type":"project"},{"authors":[],"categories":[],"content":"Estimating GPS location of a single RGB image of outdoor environment by, firstly, GPS coordinate retrieval from image classification and secondly, fine tuning the location estimate using structure from motion and and position triangulation. This application was interfaced by an Android mobile application.\n","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"487697dc64d59ade6bf7e88f1321565c","permalink":"https://srihegde.github.io/project/visloc/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/project/visloc/","section":"project","summary":"Outdoor location estimation in IIIT-Delhi campus through computer vision","tags":["computer vision","camera relocalization","machine learning"],"title":"Vision Based Outdoor Localization","type":"project"},{"authors":[],"categories":[],"content":"We present an interactive sketching interface for quick and easy designing of freeform 3D models using OpenGL and CGAL libraries in C++. The mesh construction employs Shewchuk\u0026rsquo;s algorithm (which is based on Delaunay Triangulation). The freehand interaction and mesh construction is perfomed in real-time.\n","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"58c495af465971635509f3843e6a5402","permalink":"https://srihegde.github.io/project/sketch23d/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/project/sketch23d/","section":"project","summary":"A sketching interface for rapid designing of freeform 3D models from 2D sketches","tags":["Computer Graphics","3D Modelling"],"title":"Sketch23D","type":"project"},{"authors":[],"categories":[],"content":"This project proposes to build a virtual smart campus infrastructure. A 3D interactive and immersive virtual/mixed reality environment will be designed to support geospatial services including smart navigation and telepresence. Two key aspects of the system are 3D modeling and rendering.\n","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"44ec17be3a14c58dca91019dafd2a177","permalink":"https://srihegde.github.io/project/vcp/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/project/vcp/","section":"project","summary":"Creating a virtual walk-through of IIIT-Delhi campus","tags":["computer graphics","immersive rendering","virtual reality"],"title":"Virtual Campus Project","type":"project"},{"authors":null,"categories":null,"content":"Developed a web application, that takes input from the user about his/her preferences about the specification of the machine which include - operating system, main memory space, disk(storage) space, number of cores. Then we return the IP of the machine (Virtual Machine) assigned according to the mentioned preferences, for a particular amount of time. We use KVM for VM management.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d2e9ca8e3a91248d536db1ea130e8a0","permalink":"https://srihegde.github.io/arxiv_projects/vcloud/vcloud/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/arxiv_projects/vcloud/vcloud/","section":"arxiv_projects","summary":"Cloud based Virtual Machine management system using KVM.","tags":["System Management"],"title":"Creating Cloud of Local Servers","type":"arxiv_projects"},{"authors":null,"categories":null,"content":"In this project we developed an IPv4 packet generator to simulate any network traffic the user wants. We interfaced this through a web application to provide user the provision for customizing source and destination IPs and ports,the protocol to be used and amount of that data each of the packets carry. We analyzed the most used protocols and identifying clogging points in network.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b3b0ba6dce2f7c30059ef4b7f68982f5","permalink":"https://srihegde.github.io/arxiv_projects/netpack/netpack/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/arxiv_projects/netpack/netpack/","section":"arxiv_projects","summary":"Network traffic analysis and IPv4 packet generation for custom simulation.","tags":["System Management","Network Management"],"title":"Network Traffic Generator and Packet Analyzer","type":"arxiv_projects"}]