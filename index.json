[{"authors":["admin"],"categories":null,"content":"\nWelcome to my academic homepage!\nI am a graduate researcher at the University of Maryland, College Park. My research interests lie in the intersection of 3D Vision, Computer Graphics and Deep Learning with an application domain of Augmented and Virtual Reality. I have been primarily working with Prof. Matthias Zwicker on real-time neural point cloud rendering for scientific visualization and Prof. Jia-bin Huang on photo-geometric factors in 2D generative models. Previously, I was a researcher at Innovation Labs, Tata Consultancy Services collaborating with Dr. Lovekesh Vig and Ms. Ramya Hebbalaguppe. I completed my undergrad in Computer Science and Engineering at Indraprastha Institute of Information Technology Delhi (IIITD). At IIITD, I was fortunate to work with Prof. Saket Anand and Prof. Ojaswa Sharma on interesting problems of scene reconstruction and procedural modelling.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome to my academic homepage!\nI am a graduate researcher at the University of Maryland, College Park. My research interests lie in the intersection of 3D Vision, Computer Graphics and Deep Learning with an application domain of Augmented and Virtual Reality.","tags":null,"title":"","type":"authors"},{"authors":["Srinidhi Hegde"],"categories":["Extended Lab Notes"],"content":"Ever wished you could stay on top of conference deadlines without the constant calendar checking? Well, fret no more! Two missed deadlines later (yikes!), I knew I needed a better system for keeping track of them. Enter automation!\nToday, in this article I\u0026rsquo;ll talk about the world of Twitter bots and how to build our very own conference deadline tracker bot using Python. Although there are handful of bot accounts on Twitter keeping track of conference deadlines, none of them are customized to my conference requirements in computer graphics, 3D vision, and visualization. Hence we will build our little bot friend who will scour the depths of our (shallow) data and tweet out those all-important deadlines, keeping us informed and stress-free. Let\u0026rsquo;s get started!\nShameless plug:\nüî•Please do me a favor and follow @confclock on twitter for daily updates on graphics, vision, and visualization conference deadlinesüî•\nPrerequisites: What You\u0026rsquo;ll Need A free Twitter developer account (we\u0026rsquo;ll need to chat with Twitter through code) A little Python know-how (don\u0026rsquo;t worry, it\u0026rsquo;s beginner-friendly!) Step 1: Sign Up and Get Access First things first, head over to Twitter\u0026rsquo;s developer portal and snag yourself a free developer account. You might need to write a quick blurb about why you want to use their fancy Twitter APIs (think of it as your bot\u0026rsquo;s resume). Congratulations(if you got the access)! You can pull upto 1500 posts per month from Twitter if it is a free developer account.\nStep 2: Project Time! Once you\u0026rsquo;re in, you\u0026rsquo;ll see a new project with a random name. Give it a snazzy name that reflects your bot\u0026rsquo;s purpose (For e.g., \u0026ldquo;GraphViz DL\u0026rdquo; for our case or something drippy - \u0026ldquo;Deadline Slayer 3000\u0026rdquo;). There are two things here a project and an app. You can have projects in your account. Also you can have multiple apps under a project and each app servers a different purpose.\nStep 3: Building Your Bot Account Here comes the fun part: creating your bot account! You can refer to this nifty article that talks about creating a new twitter account by \u0026ldquo;hacking\u0026rdquo; the twitter login checker. Unfortunately, if none of that works for you then you\u0026rsquo;ll need a separate email address or phone number. So dust off an old email or grab a temporary phone number service.\nStep 4: Talk Twitter Talk (Authentication!) Now, we need to get our bot authorized to use Twitter\u0026rsquo;s superpowers. This involves generating some special codes or tokens and saving them securely. Think of them as secret handshakes that unlock Twitter\u0026rsquo;s features for your bot.\nFor this go to the Settings -\u0026gt; User Authentication Settings and update the app permissions, type of app, and the app info sections as per your requirements. I have used the following settings for my bot:\nNow to generate and save the tokens, go to the Keys and Tokens tab in your Twitter developer account. You\u0026rsquo;ll see five keys here: API key, API secret key, Bearer token, Access token, and Access token secret. These are the secret handshakes we need to talk to Twitter (which uses 3-legged OAuth Flow behind the scenes).\nStep 5: Python Power! Writing the Script This is where the Python magic happens! We\u0026rsquo;ll write a script that uses a cool library called \u0026ldquo;tweepy\u0026rdquo; to chat with Twitter (with v2 endpoint APIs) and send out those deadline tweets. We\u0026rsquo;ll also set up a system to read conference information from a file (like a CSV spreadsheet).\nHere\u0026rsquo;s a simple script to get you started:\nimport tweepy as tpy import os import csv from datetime import datetime from typing import Tuple, Dict from dotenv import load_dotenv # Authentication tokens root_path = './' load_dotenv(os.path.join(root_path, '.env')) consumer_key = os.getenv(\u0026quot;CONSUMER_KEY\u0026quot;) consumer_secret = os.getenv(\u0026quot;CONSUMER_SECRET\u0026quot;) access_token = os.getenv(\u0026quot;ACCESS_TOKEN\u0026quot;) access_token_secret = os.getenv(\u0026quot;ACCESS_TOKEN_SECRET\u0026quot;) # Authenticate to Twitter client = tpy.Client(consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=access_token, access_token_secret=access_token_secret) # Tweet the message client.create_tweet(\u0026quot;Hello, Twitter!\u0026quot;) Don\u0026rsquo;t be a noob and store the keys in the script itself. Instead, use a .env file to store the keys (eg. COMSUMER_KEY=xxxx\u0026hellip;xx, etc.) and load them into your script using load_dotenv module. This way, your keys stay secure and your code is modular.\nStep 6: Tweeting from the Bot Account Here\u0026rsquo;s the tricky bit. We\u0026rsquo;ve written a script that can tweet, but it\u0026rsquo;s currently tweeting from your main account. We need to link it to your bot account so the deadlines get posted there. There are a couple of ways to do this, and we\u0026rsquo;ll be using a method with a fancy name: OAuth 1.0a User Context.\nThis involves a bit of back-and-forth between your code and Twitter to get the necessary permissions. Long story short, you\u0026rsquo;ll need to generate an authentication link for your bot account to access the app\u0026rsquo;s functionality (remember app is in your main account), click on the link, and access the secret tokens of your bot account to be used in the app to tweet on bot\u0026rsquo;s behalf. Don\u0026rsquo;t worry, the instructions will guide you through it step-by-step.\nFirstly, we need to generate the an authentication link for the bot account to access the app\u0026rsquo;s functionality. This can be done by using the following code:\nconsumer_key = os.getenv(\u0026quot;CONSUMER_KEY\u0026quot;) consumer_secret = os.getenv(\u0026quot;CONSUMER_SECRET\u0026quot;) oauth1_user_handler = tpy.OAuth1UserHandler( consumer_key=consumer_key, consumer_secret=consumer_secret, callback='https://twitter.com') # The following link will ask you to authorize the app to use your account. # Authorize and then you will be redirected to twitter.com with an oauth_verifier # field in the URL. Pass that in the prompt of the next cell. print(oauth1_user_handler.get_authorization_url(signin_with_twitter=True)) Here consumer key and secret are from the main account. This script spits the authentication link (which will look something like - https://twitter.com/home?oauth_token=\u0026lt;some_oauth_token\u0026gt;\u0026amp;oauth_verifier=\u0026lt;some_oauth_verifier\u0026gt;). Click on the link and authorize the app to use your account. You will be redirected to twitter.com with an oauth_verifier field in the URL. Copy that and pass it in the next cell below:\nverifier = input('Verifier: ') # Save the following tokens securely in .env file bot1_access_token, bot1_access_token_secret = oauth1_user_handler.get_access_token(verifier) # Authenticate to Twitter with the bot account client = tpy.Client(consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=bot1_access_token, access_token_secret=bot1_access_token_secret) Now you have the bot account\u0026rsquo;s access tokens. Save them securely in the .env file. Now you can tweet from the bot account using tweepy\u0026rsquo;s create_tweet() function. Also note that this approach requires too much manual intervention. You can automate this setup by creating some sort of web application that would parse the url and retrieve the oauth_verifier. But I did this because this is a one-time process and we are saving the required tokens of the bot account.\nStep 7: Feeding the Beast (Conference Deadline Data) Now that the tweeting part is figured out, we need to tell our bot what to tweet about! Create a file (like a CSV) containing conference names and deadlines. This will be the bot\u0026rsquo;s treasure trove of information.\nI am storing my data in \u0026lt;conference_name, conference_deadline\u0026gt; format.The Python script will read this file, transform it into a tweet-worthy message, and send it out to the Twitterverse. Here\u0026rsquo;s a simple example of how you can read the data from a CSV file:\ndef extract_msg(msg_dict: Dict, ulimit: int=151) -\u0026gt; Tuple[str, Dict]: msg = '' delkeys = [] for cname, cinfo in msg_dict.items(): if cinfo['days'] == 0: msg += f\u0026quot;{cname}: Today! Good Luck!\\n\u0026quot; elif cinfo['days'] \u0026gt; 0 and cinfo['days'] \u0026lt; ulimit: msg += f\u0026quot;{cname}: {cinfo['days']} days\\n\u0026quot; elif cinfo['days'] \u0026lt; 0: delkeys.append(cname) for k in delkeys: del msg_dict[k] return msg, msg_dict def get_deadlines_msg(csvfile: str) -\u0026gt; str: with open(csvfile, newline='') as f: reader = csv.reader(f) deadlines = list(reader) date_frmt = '%d-%m-%Y' msg_dict = {} for dl in deadlines: cname = dl[0].strip() date = dl[1].strip() today = datetime.now().strftime(date_frmt) days_left = datetime.strptime(date, date_frmt) - datetime.strptime(today, date_frmt) msg_dict[cname] = {'date': date, 'days':days_left.days} msg_dict = {k: v for k, v in sorted(msg_dict.items(), key=lambda item: item[1]['days'])} msg, msg_dict = extract_msg(msg_dict) # Remove the passed deadline from the csv file with open(csvfile, 'w', newline='') as f: writer = csv.writer(f) for cname, days_left in msg_dict.items(): date = days_left['date'] writer.writerow([cname, date]) return msg csvfile = 'deadlines.csv' msg = get_deadlines_msg(csvfile) Step 8: Scheduling the Tweets We don\u0026rsquo;t want to babysit our bot, so let\u0026rsquo;s schedule it to tweet automatically every day. We\u0026rsquo;ll be using a cloud platform called PythonAnywhere to host our Python application. They offer a free plan that\u0026rsquo;s perfect for this.\nFirstly, create an account on this site and setup the environment on the server. Free account gives you enough space and compute time to run this app couple of times a day. For this we will compile all the above code into a python script, called app.py, and upload it to the PythonAnywhere server along with .env and our CSV data file.\nNext, create a virtual environment and install the required packages. You can do this by running the following commands in the bash console on the server:\nmkvirtualenv dlbot --python=/usr/bin/python3.10 --seeder=pip workon dlbot pip install tweepy python-dotenv Now, let\u0026rsquo;s schedule a task in the Tasks tab that runs our app.py script exactly once at a fixed time of the day. You can do this by adding a new task with the following task command:\nworkon dlbot; python /\u0026lt;your path to project folder\u0026gt;/app.py That\u0026rsquo;s it - your bot is now ready to tweet out those all-important deadlines every day! You can check the logs to see if the tweets are being sent out as expected. If you want to test the bot, you can run the script manually by setting a close enough time to run the script.\nStep 9: üî•üî• Bonus Level üî•üî• - Automating Deadline Gathering (For the super-ambitious coders) Here is a challenge! Imagine a world where your bot finds conference deadlines all by itself! We can explore using a fancy Large Language Models (LLMs) to scrape conference websites and extract deadlines. This is some next-level bot building!\nHint: You can use tools like Tavily (https://tavily.com) which run over ChatGPT-4.0 APIs to produce detailed research reports given a task. You can prompt engineer the model to extract deadlines from the conference websites effectively.\nIf you made it till here, you are awesome! Now check out (and follow üôèüôèüôè) @confclock on twitter and see what it can do.\nPeace out! ü§ñüöÄ\nResources: Now if you are thinking about where to start, do not worry. I have got you covered! Here is the link to my GitHub repository where you can find all the code used in this blog (and more).\nThe world of Twitter bots can be a bit overwhelming, so here are some helpful resources to guide you on your journey:\nWhile a tad outdated, the core concepts are still useful Useful discussion 1 Useful discussion 2 Schedule tasks on PythonAnywhere Youtube tutorial With a little dedication and these handy steps, you\u0026rsquo;ll be well on your way to building your very own conference deadline tracking Twitter bot. No more scrambling to meet deadlines ‚Äì your bot will be your hero!\n","date":1715472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715472000,"objectID":"fab741aa5c3041994303f93c59ab4ce8","permalink":"https://srihegde.github.io/post/dlbot/","publishdate":"2024-05-12T00:00:00Z","relpermalink":"/post/dlbot/","section":"post","summary":"Ever wished you could stay on top of conference deadlines without the constant calendar checking? Well, fret no more! Two missed deadlines later (yikes!), I knew I needed a better system for keeping track of them.","tags":[],"title":"Never Miss a Deadline Again","type":"post"},{"authors":["Srinidhi Hegde"],"categories":["Off-road Musings"],"content":"In case you find art and geometry fascinating, here is my collection of some snaps that held my gaze.\n(Subject to Copyright ¬© Srinidhi Hegde 2022.)\n","date":1657325097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657325097,"objectID":"3f5df4e7c6746eb281f3a314d89e79d5","permalink":"https://srihegde.github.io/post/art_album/","publishdate":"2022-07-08T19:04:57-05:00","relpermalink":"/post/art_album/","section":"post","summary":"In case you find art and geometry fascinating, here is my collection of some snaps that held my gaze.\n(Subject to Copyright ¬© Srinidhi Hegde 2022.)","tags":[],"title":"Kaleidoscope","type":"post"},{"authors":["Srinidhi Hegde"],"categories":["Off-road Musings"],"content":"Peering out of the window on a lazy Sunday afternoon I noticed something peculiar about the house across the street. A break of 3000 odd house numbers between my neighbor and my house didn‚Äôt add up to me. My neighborhood was a tiny enclosure amidst the crotched bustling highways of Maryland. How in the world could this small patch of land fit all these houses? Revelations and tragedy always strike with an element of surprise. But what is worse? Both struck me simultaneously as I realized that these unaccounted houses were the progeny of my amnesic memory which threw up an incorrect house number for my new residence. However, I don‚Äôt blame myself for this. This episode happened while I was settling in a foreign land having stuck to a single house number all my life.\nBut wait, why is this a tragedy, you ask? Well, this was the incorrect address that I typed in while ordering my brand new bicycle (or bike, as Americans call it) from Walmart. Regaining my composure, I first checked if this incorrect address actually existed, so that I can just collect it from there. But remember, it was a tiny neighborhood and I had directed my package to a house that was 3000 houses away and literally nowhere on earth. So I did the next natural thing that a panic-struck customer does. Calling the Walmart support center. A kind woman answered my call and as I poured out my vexation she looked up my order details. After several rounds of information exchange, we figured out that we could not cancel the order as it was an expensive purchase and it was on its way to delivery. ‚ÄúCan the delivery service at least deliver my bike to the updated location?‚Äù, I asked. After a doubtful pause and a quick consultation with her supervisor, she announced, ‚ÄúYes, you can expect the package within 2 days‚Äù. And with that, I gave a sigh of relief. But little did I know how far I was from the relief!\nThree days had passed and there was still no sign of my bike and my paranoia was ballooning. Being a faithful paranoid customer, I dialed up the support center again. This time I heard a different male voice from the other side. And this meant I had to revisit my predicament and also the solution which we had arrived at. After hearing my account, the support staff pondered for a while, verified the purchase records, and nonchalantly declared that my order was canceled and the refund was initiated. After hearing this unpleasant news, I hung up the call and immediately rushed to reorder the same bike. But the product had run out of stock! Having surveyed gazillion bikes online, gauging their tire size, off-road riding, price, ‚Ä¶ (replace with any filter you can think of, yes including gender), I had lost it all. With a heavy heart, I settled for the next best bike on my surveyed bike list. And I promptly ordered this bike online, of course, with the right address this time.\nThe next morning as I got ready to leave for my early morning classes in the university, I saw a huge tattered cardboard box blocking my house entrance. And what‚Äôs more? The box was addressed for me and the first thing that clicked me was the new bike that I ordered the day before. Boy was I impressed by Walmart‚Äôs supersonic delivery. Since I was getting late for my lecture, controlling my excitement, I shoved the hefty package inside the house and rushed for the classes. To be honest, I could not concentrate on the lecture that day. Biking to campus, swishing past the pedestrians, minting precious time for myself, and thoughts alike kept feeding the butterflies in my stomach. Yes, I know I didn‚Äôt care much about its features, for the bike was the cheapest and the most basic one, probably in the entire campus, that could just take me places. In short, I was fixated on the tattered box in my verandah.\nAs the class ended that day, it was probably one of the fastest walks (actually interspersed bursts of sprints) that I had to my home. I unlocked the front door of my house, rushed towards the box picking the nearby scissors. Snip! Snip! I effortlessly tore open the already tattered box. But what do I discover here? This was the bike that I had ordered before ordering my replacement bike that was inferior to my first bike. Did you read too many bikes? Don‚Äôt worry. Let us call the first bike (that was canceled) the red bike and the second bike (the replacement) the blue bike and I had received the red bike in the package. Well, this was a confusing situation for me. To aggravate the matters, I woke up to another package the next day, waiting for me. I carefully went over the same exercise of tearing-up-the-tattered-cardboard. What do I find here? Yes, you guessed it right. It was the blue bike (yikes! I have 2 bikes!). That was the moment I was lost somewhere amidst the confusion of joy, awe, anxiety(over returning and refund process), and, anger (directed at Walmart and partly to myself).\nAfter several unsuccessful attempts to bring me to my senses, it took me three weeks to accept what had happened. I had neither received the refund for the red bike (which was anticipated in 8-10 business days) nor did I hear anything from Walmart till then. And I was the unexpected owner of two bikes and I was presented with the choice of red cycle versus the blue cycle. Although it sounds like the iconic red and blue pills of Keanu Reaves‚Äô Matrix. But making a decision here was not that hard. I decided to keep the red one but what to do with the blue bike? Luckily for me, one of my housemates did not have a bike and it took me a little bit of convincing to sell my bike to him (how I marketed my product could be a topic for a separate blog post).\nNow, if you are wondering if this was my ‚Äúhappily ever after‚Äù moment, then hold your horses. After a week, almost a month had passed since the refund for my red bike was initiated. It seems like the stars aligned well that day and I got my refund. So now I had two bikes one of which was free! Finally, I was relieved. But in my hindsight, I had a sinister feeling that something was not right. Moreover, one of my friends, after hearing my case, subtly prompted that this seems illegal and I could be up for some trouble. Firing up my researcher instincts, I scoured through the internet to verify if I am the unlucky one. Well to my surprise, not one or two but hundreds of cases like this crop up every year due to logistical goof-ups. To make the matters worse, I also read that some state jurisdictions consider this as theft and a 6th-degree felony leading up to a year‚Äôs prison term! Instantly, my relief switched to paranoia. Do I have to return my bike? My favorite red bike? Or even worse, will I be incarcerated? To remedy this situation, I dialed the Walmart service center once again and apologetically explained myself to the support staff. After a momentary pause, which seemed like eons to me, I heard the three golden words - ‚ÄúJust keep it!‚Äù. No questions asked. Period. By this time, I was so used to my paranoia that I breathed a sigh of relief anyway. I finally owned a freebie bike legally. How cool is that?\nEven now, just to give my creativity some rest, I recycle this story as icebreakers in conversations during parties, dinners, and even in classrooms! (What‚Äôs worse, I can refer to this blog from now on.) My audience generally compliments my honesty and some are even surprised that I called the support center to clarify. But little do they know that this should be credited to my guilt and paranoia. Looking back at this whole episode, I do and don‚Äôt blame Walmart for this episode. But I can‚Äôt complain either. On the flip side, I always cherish how I came so close to stealing a bike and getting away with it!\nIf you have made it so far, I thank you for your awesomeness.\n","date":1641168297,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168297,"objectID":"d4480befcb866daa788de2b632037ad5","permalink":"https://srihegde.github.io/post/bikesaga/","publishdate":"2022-01-02T19:04:57-05:00","relpermalink":"/post/bikesaga/","section":"post","summary":"Peering out of the window on a lazy Sunday afternoon I noticed something peculiar about the house across the street. A break of 3000 odd house numbers between my neighbor and my house didn‚Äôt add up to me.","tags":[],"title":"The \"Bi\"-Cycle Saga","type":"post"},{"authors":["M. Gwilliam","S. Hegde","L Tinubu","A Hanson"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"d5e7a475a49de95dfe3c9d324ea92c8a","permalink":"https://srihegde.github.io/publication/iccv2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/iccv2021/","section":"publication","summary":"Many existing works have made great strides towards reducing racial bias in face recognition. However, most of these methods attempt to rectify bias that manifests in models during training instead of directly addressing a major source of the bias, the dataset itself. Exceptions to this are BUPT-Balancedface/RFW and Fairface, but these works assume that primarily training on a single race or not racially balancing the dataset are inherently disadvantageous. We demonstrate that these assumptions are not necessarily valid. In our experiments, training on only African faces induced less bias than training on a balanced distribution of faces and distributions skewed to include more African faces produced more equitable models. We additionally notice that adding more images of existing identities to a dataset in place of adding new identities can lead to accuracy boosts across racial categories. Our code is available at https://github.com/j-alex-hanson/rethinking-race-face-datasets.","tags":[],"title":"Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets","type":"publication"},{"authors":["A. Khattar","S. Hegde","R. Hebbalaguppe"],"categories":null,"content":"","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"059b347930c2bbdaf20a803bd586576a","permalink":"https://srihegde.github.io/publication/cvprw2021/","publishdate":"2021-06-19T00:00:00Z","relpermalink":"/publication/cvprw2021/","section":"publication","summary":"Multi-task learning (MTL) is a learning paradigm that aims at joint optimization of multiple tasks using a single neural network for better performance and generalization. In practice, MTL rests on the inherent assumption of availability of common datasets with ground truth labels for each of the downstream tasks. However, collecting such a common annotated dataset is laborious for complex computer vision tasks such as the saliency estimation which would require the eye fixation points as the ground truth data. To this end, we propose a novel MTL framework in the absence of common annotated dataset for joint estimation of important downstream tasks in computer vision - object detection and saliency estimation. Unlike many state-of-the-art methods, that rely on common annotated datasets for training, we consider the annotations from different datasets for jointly training different tasks, calling this setting as cross-domain MTL. We adapt MUTAN framework to fuse features from different datasets to learn domain invariant features capturing the relatedness of different tasks. We demonstrate the improvement in the performance and generalizability of our MTL architecture. We also show that the proposed MTL network offers a 13% reduction in memory footprint due to parameter sharing between the related tasks.","tags":[],"title":"Cross-Domain Multi-task Learning for Object Detection and Saliency Estimation","type":"publication"},{"authors":["S. Yalburgi","T. Dash","R. Hebbalaguppe","S. Hegde","A. Srinivasan"],"categories":null,"content":"","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"08737ab22db38567412fd45a2fdd9974","permalink":"https://srihegde.github.io/publication/esann2020/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/publication/esann2020/","section":"publication","summary":"In this paper we introduce Iterative Knowledge Distillation (IKD), the process of successively minimizing models based on the Knowledge Distillation (KD) approach in Hinton et al. We study two variations of IKD, called parental- and ancestral- training. Both use a single-teacher, and result in a single-student model: the differences arise from which model is used as a teacher. Our results provide support for the utility of the IKD procedure, in the form of increased model compression, without significant losses in predictive accuracy. An important task in IKD is choosing the right model(s) to act as a teacher for a subsequent iteration. Across the variations of IKD studied, our results suggest that the most recent model constructed (parental-training) is the best single teacher for the model in the next iteration. This result suggests that training in IKD can proceed without requiring us to keep all models in the sequence.","tags":[],"title":"An Empirical Study of Iterative Knowledge Distillation for Neural Network Compression","type":"publication"},{"authors":["S. Hegde","R. Prasad","R. Hebbalaguppe","V. Kumar"],"categories":null,"content":"","date":1588550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588550400,"objectID":"10846a107312909bfeea26f6a647a5e6","permalink":"https://srihegde.github.io/publication/icassp2020/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/publication/icassp2020/","section":"publication","summary":"The holy grail in deep neural network research is porting the memory- and computation-intensive network models on embedded platforms with a minimal compromise in model accuracy. To this end, we propose a novel approach, termed as Variational Student, where we reap the benefits of compressibility of the knowledge distillation (KD) framework, and sparsity inducing abilities of variational inference (VI) techniques. Essentially, we build a sparse student network, whose sparsity is induced by the variational parameters found via optimizing a loss function based on VI, leveraging the knowledge learnt by an accurate but complex pre-trained teacher network. Further, for sparsity enhancement, we also employ a Block Sparse Regularizer on a concatenated tensor of teacher and student network weights. We demonstrate that the marriage of KD and the VI techniques inherits compression properties from the KD framework, and enhances levels of sparsity from the VI approach, with minimal compromise in the model accuracy. We benchmark our results on LeNet MLP and VGGNet (CNN) and illustrate a memory footprint reduction of 64x and 213x on these MLP and CNN variants, respectively, without a need to retrain the teacher network. Furthermore, in the low data regime, we observed that our method outperforms state-of-the-art Bayesian techniques in terms of accuracy.","tags":[],"title":"Variational Student: Learning Compact and Sparser Networks in Knowledge Distillation Framework","type":"publication"},{"authors":["S. Hegde","J. Maurya","A. Kalkar","R. Hebbalaguppe"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"33d131c67ca378e17eb6fe1371966a3e","permalink":"https://srihegde.github.io/publication/wacv2020/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/wacv2020/","section":"publication","summary":"","tags":[],"title":"SmartOverlays: A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces","type":"publication"},{"authors":[],"categories":[],"content":"Compressing the memory-intensive DNN models with a minimal compromise in model accuracy using variational methods and knowledge distillation. Another part of projects involves proposing theoretical guarantees on the knowledge distillation models for efficient neural architecture search.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"831d1ad0dbf77e63346af26bafe8891e","permalink":"https://srihegde.github.io/project/nncomp/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/nncomp/","section":"project","summary":"Compressing deep neural network models for memory constrained devices","tags":["neural networks","deep learning","optimization","variational inference"],"title":"Deep Neural Network Compression","type":"project"},{"authors":[],"categories":[],"content":"Automating the animation through learning and transferring motion cues from the real RGB-D videos to virtual 3D meshes. We use graph based structure correspondences to map the motion between the two 3D entities such as point cloud and 3D meshes.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"7f0371b66ae970b1302cb3fa6604babd","permalink":"https://srihegde.github.io/arxiv_projects/animation/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/arxiv_projects/animation/","section":"arxiv_projects","summary":"Automating animation through motion transfer from real scenes.","tags":["computer graphics","computer vision","animation","machine learning"],"title":"Reverse VooDoo","type":"arxiv_projects"},{"authors":[],"categories":[],"content":"A novel method for the placement of labels corresponding to objects of interest in images/videos/live feeds that is non-intrusive, relevant and temporally coherent. We employ different techniques ranging from search space optimization to visual saliency based neural network framework.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"2cd5acc906258452e0e9aa258e8d59ba","permalink":"https://srihegde.github.io/project/smartov/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/project/smartov/","section":"project","summary":"A non-intrusive, relevant and temporally coherent overlay placement technique","tags":["situated visualization","augmented reality","computer vision","visual saliency"],"title":"SmartOverlays","type":"project"},{"authors":["G. Garg","S. Hegde","R. Perla","V. Jain","L. Vig","R. Hebbalaguppe"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"15188d8fad6a9fe827f1771bd7ec8392","permalink":"https://srihegde.github.io/publication/eccv2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/eccv2018/","section":"publication","summary":"Hand gestures form a natural way of interaction on Head-Mounted Devices (HMDs) and smartphones. HMDs such as the Microsoft HoloLens and ARCore/ARKit platform enabled smartphones are expensive and are equipped with powerful processors and sensors such as multiple cameras, depth and IR sensors to process hand gestures. To enable mass market reach via inexpensive Augmented Reality (AR) headsets without built-in depth or IR sensors, we propose a real-time, in-air gestural framework that works on monocular RGB input, termed, DrawInAir.DrawInAir uses fingertip for writing in air analogous to a pen on paper. The major challenge in training egocentric gesture recognition models is in obtaining sufficient labeled data for end-to-end learning. Thus, we design a cascade of networks, consisting of a CNN with differentiable spatial to numerical transform (DSNT) layer, for fingertip regression, followed by a Bidirectional Long Short-Term Memory(Bi-LSTM), for a real-time pointing hand gesture classification. We highlight how a model,that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better overend-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model. We showthat the framework takes 1.73s to run end-to-end and has a low memory footprint of 14MB while achieving an accuracy of 88.0% on egocentric video dataset.","tags":[],"title":"DrawInAir: A Lightweight Gestural Interface Based on Fingertip Regression","type":"publication"},{"authors":["N. Rakholia","S. Hegde","R. Hebbalaguppe"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"4dc7d3c2966a164272530172b993be87","permalink":"https://srihegde.github.io/publication/icip2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/icip2018/","section":"publication","summary":"Textual overlays/labels add contextual information in Augmented Reality (AR) applications. The spatial placement of labels is a challenging task due to constraints that labels (i) are not occluding the object/scene of interest, and, (ii) are optimally placed for better interpretation of scene. To this end, we present a novel method for optimal placement of labels for AR. We formulate this method by an objective function that minimizes both occlusion with visually salient regions in scenes of interest, and the temporal jitter for facilitating coherence in real-time AR applications. The main focus of proposed algorithm is real-time label placement on low-end android phones/tablets. The sophisticated state-of-the-art algorithms for optimal positioning of textual label work only on the images and are often inefficient for real-time performance on those devices. We demonstrate the efficiency of our method by porting the algorithm on a smart-phone/tablet. Further, we capture objective and subjective metrics to determine the efficacy of the method; objective metrics include computation time taken for determining the label location and Label Occlusion over Saliency (LOS) score over salient regions in the scene. Subjective metrics include position, temporal coherence in the overlay, color and responsiveness.","tags":[],"title":"Where To Place: A Real-Time Visual Saliency Based Label Placement for Augmented Reality Applications","type":"publication"},{"authors":[],"categories":[],"content":"Hand gesture classification through fingertip coordinate regression in a temporal model for touch-less interactions in AR. We highlight how a model, that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better over end-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model.\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"95083189b28bb1a09399d31cc3c5e338","permalink":"https://srihegde.github.io/project/drawinair/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/project/drawinair/","section":"project","summary":"A hand gesture based in-air touch-less interaction technique for AR applications","tags":["hand gestures","computer vision","augmented reality","deep learning"],"title":"Draw In Air","type":"project"},{"authors":[],"categories":[],"content":"Employing CNNs for an end-to-end reconstruction of the indoor scenes through camera relocalization, through PoseNet, and depth estimation, through multi-scale fully convolutional network, from a single RGB image during inference and registering the 3D reconstructed patches through iterative closest point algorithm. A portion of the dataset collected during the project is also released.\n","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"f109aed8fe02906bc2c731735cb883dc","permalink":"https://srihegde.github.io/project/btp/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/project/btp/","section":"project","summary":"Developing an end to end pipeline for automatic generation of 3D models of indoor scenes.","tags":["deep learning","3D reconstruction","camera relocalization","computer vision"],"title":"Robust 3D Reconstruction of Indoor Scenes using Deep Learning","type":"project"},{"authors":["S. Hegde","R. Perla","R. Hebbalaguppe","E. Hassan"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"03671fcde735b5d8d799d0ebf19df59e","permalink":"https://srihegde.github.io/publication/ismar2016/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/ismar2016/","section":"publication","summary":"The existing, sophisticated AR gadgets in the market today are mostly exorbitantly priced. This limits their usage for the upcoming academic research institutes and also their reach to the mass market in general. Among the most popular and frugal head mounts, Google Cardboard (GC) and Wearality are video-see-through devices that can provide immersible AR and VR experiences with a smartphone. Stereo-rendering of camera feed and overlaid information on smartphone helps us experience AR with GC. These frugal devices have limited user-input capability, allowing user interactions with GC such as head tilting, magnetic trigger and conductive lever. Our paper proposes a reliable and intuitive gesture based interaction technique for these frugal devices. The hand gesture recognition employs the Gaussian Mixture Models (GMM)based on human skin pixels and tracks segmented foreground using optical flow to detect hand swipe direction for triggering a relevant event. Real-time performance is achieved by implementing the hand gesture recognition module on a smartphone and thus reducing the latency. We augment real-time hand gestures as new GC‚Äôs interface with its evaluation done in terms of subjective metrics and with the available user interactions in GC.","tags":[],"title":"GestAR: Real Time Gesture Interaction for AR with Egocentric View","type":"publication"},{"authors":[],"categories":[],"content":"This work develops a distributed fault tolerant area coverage algorithm, resulting in quick detection of the faulty agent under limited communication constraints and redistributes the area without conflicts.\n","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"e500c976dc3bf8d2d7047c9d11fe6935","permalink":"https://srihegde.github.io/project/mas/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/project/mas/","section":"project","summary":"Analysed Multi-Agent area coverage problem for surveillance purposes.","tags":["multi-agent systems","area coverage","scheduling","robotics"],"title":"Distributed Fault Tolerant Multi-Robot Area Coverage Under Limited Communication Ranges","type":"project"},{"authors":[],"categories":[],"content":"Realistic models of vegetations are very essential piece of immersive virtual environment simulations. We develop a novel technique to convert a single captured image of a vegetation to a 3D model using L-Systems, a context-free grammar that we adapt to procedurally model vegetation. We also propose a pipeline that is semi-automated with manual interventions for accurate identification of tree branches and trunk.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7ba1e13a3b867f0d96e2ccdc01f65e5d","permalink":"https://srihegde.github.io/project/lsys/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/lsys/","section":"project","summary":"Procedural modelling of vegetation using context-free grammar","tags":["Computer Graphics","3D Modelling","Context-free Grammar"],"title":"Modelling Vegetation with L-Systems Using an Image","type":"project"},{"authors":[],"categories":[],"content":"Estimating GPS location of a single RGB image of outdoor environment by, firstly, GPS coordinate retrieval from image classification and secondly, fine tuning the location estimate using structure from motion and and position triangulation. This application was interfaced by an Android mobile application.\n","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"8f2951175dba63774f20f2fe2c3834cd","permalink":"https://srihegde.github.io/arxiv_projects/visloc/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/arxiv_projects/visloc/","section":"arxiv_projects","summary":"Outdoor location estimation in IIIT-Delhi campus through computer vision","tags":["computer vision","camera relocalization","machine learning"],"title":"Vision Based Outdoor Localization","type":"arxiv_projects"},{"authors":[],"categories":[],"content":"We present an interactive sketching interface for quick and easy designing of freeform 3D models using OpenGL and CGAL libraries in C++. The mesh construction employs Shewchuk\u0026rsquo;s algorithm (which is based on Delaunay Triangulation). The freehand interaction and mesh construction is perfomed in real-time.\n","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"58c495af465971635509f3843e6a5402","permalink":"https://srihegde.github.io/project/sketch23d/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/project/sketch23d/","section":"project","summary":"A sketching interface for rapid designing of freeform 3D models from 2D sketches","tags":["Computer Graphics","3D Modelling"],"title":"Sketch23D","type":"project"},{"authors":[],"categories":[],"content":"This project proposes to build a virtual smart campus infrastructure. A 3D interactive and immersive virtual/mixed reality environment will be designed to support geospatial services including smart navigation and telepresence. Two key aspects of the system are 3D modeling and rendering.\n","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"44ec17be3a14c58dca91019dafd2a177","permalink":"https://srihegde.github.io/project/vcp/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/project/vcp/","section":"project","summary":"Creating a virtual walk-through of IIIT-Delhi campus","tags":["computer graphics","immersive rendering","virtual reality"],"title":"Virtual Campus Project","type":"project"},{"authors":null,"categories":null,"content":"Developed a web application, that takes input from the user about his/her preferences about the specification of the machine which include - operating system, main memory space, disk(storage) space, number of cores. Then we return the IP of the machine (Virtual Machine) assigned according to the mentioned preferences, for a particular amount of time. We use KVM for VM management.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d2e9ca8e3a91248d536db1ea130e8a0","permalink":"https://srihegde.github.io/arxiv_projects/vcloud/vcloud/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/arxiv_projects/vcloud/vcloud/","section":"arxiv_projects","summary":"Cloud based Virtual Machine management system using KVM.","tags":["System Management"],"title":"Creating Cloud of Local Servers","type":"arxiv_projects"},{"authors":null,"categories":null,"content":" 17th July 2023: Our work \u0026ldquo;Diffusion Models Beat GANs on Image Classification\u0026rdquo; published on Arxiv. Check it out here.\n21st May 2023: Graduated from UMD with masters in Computer Science!\n18th Oct 2021: My co-authored work \u0026ldquo;Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets\u0026rdquo; won the best paper running up award at ICCV (HTCV \u0026lsquo;21)! Congrats to all the co-authors.\n1st Aug 2021: My co-authored work \u0026ldquo;Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets\u0026rdquo; accepted at ICCV (HTCV \u0026lsquo;21) as a oral paper. My first publication as a UMD student!\n25th May 2021: I have started my grad school at University of Maryland, College Park.\n13th Apr 2021: My co-authored work \u0026ldquo;Cross-Domain Multi-task Learning for Object Detection and Saliency Estimation\u0026rdquo; accepted at CVPR (CLVISION \u0026lsquo;21) as a poster.\n14th Feb 2021: Received TCS Citation Award for the second time for outstanding contributions to the organization through publications.\n20th July 2020: Invited as a reviewer for IEEE Transactions on Circuits and Systems for Video Technology (TCSVT, impact factor: 3.599). My first journal reviewing experience!\n21st Mar 2020: TreasAR Hunt - an AR based treasure hunt - organized for Re.Fresh 2020 at TCS Innovation Labs New Delhi. The source code is now available. Check it out!\n24th Jan 2020: Variational Student: Our work on neural network compression through sparsification in Knowledge Distillation framework got accepted as an \u0026ldquo;oral presentation\u0026rdquo; at ICASSP 2020 to be held at Barcelona, Spain.\n24th Jan 2020: IKD: Our work on empirical analysis of iterative knowledge distillation methods got accepted at ESANN 2020 to be held at Bruges, Belgium.\n17th Sep 2019: SmartOverlays: Our work on situated visualization in AR and video application got accepted at WACV 2020 to be held at Aspen, Colorado.\n6th Jul 2019: Attended EEML Summer School 2019, Bucharest, Romania. My experience was awesome!\n15th March 2019: Selected for EEML Summer School 2019. Travelling to Bucharest, Romania in July.\n21st Jan 2019: TCS-PanIIT Conclave 2019- Hackathon Update: Mentored teams finished at 3rd and 4th positions in the event. Kudos to the team members!\n14th Jan 2019: Selected as mentor for PanIIT Hackathon at TCS-PanIIT Conclave 2019 organized by TCS and IIT Delhi.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9eb50f9088083bebcb7e4cf99e22b9ed","permalink":"https://srihegde.github.io/news/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"17th July 2023: Our work \u0026ldquo;Diffusion Models Beat GANs on Image Classification\u0026rdquo; published on Arxiv. Check it out here.\n21st May 2023: Graduated from UMD with masters in Computer Science!","tags":null,"title":"Events Archive","type":"page"},{"authors":null,"categories":null,"content":"In this project we developed an IPv4 packet generator to simulate any network traffic the user wants. We interfaced this through a web application to provide user the provision for customizing source and destination IPs and ports,the protocol to be used and amount of that data each of the packets carry. We analyzed the most used protocols and identifying clogging points in network.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b3b0ba6dce2f7c30059ef4b7f68982f5","permalink":"https://srihegde.github.io/arxiv_projects/netpack/netpack/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/arxiv_projects/netpack/netpack/","section":"arxiv_projects","summary":"Network traffic analysis and IPv4 packet generation for custom simulation.","tags":["System Management","Network Management"],"title":"Network Traffic Generator and Packet Analyzer","type":"arxiv_projects"}]