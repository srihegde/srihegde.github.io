[{"authors":["admin"],"categories":null,"content":"\nWelcome to my academic homepage!\nI am a Computer Science graduate student at the University of Maryland, College Park. My research interests lie in the intersection of 3D Vision, Computer Graphics and Deep Learning with an application domain of Augmented and Virtual Reality. I have been primarily working with Prof. Matthias Zwicker on real-time neural point cloud rendering for scientific visualization and Prof. Jia-bin Huang on photo-geometric factors in 2D generative models. Previously, I was a researcher at Innovation Labs, Tata Consultancy Services collaborating with Dr. Lovekesh Vig and Ms. Ramya Hebbalaguppe. I completed my undergrad in Computer Science and Engineering at Indraprastha Institute of Information Technology Delhi (IIITD). At IIITD, I was fortunate to work with Prof. Saket Anand and Prof. Ojaswa Sharma on interesting problems of scene reconstruction and procedural modelling.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome to my academic homepage!\nI am a Computer Science graduate student at the University of Maryland, College Park. My research interests lie in the intersection of 3D Vision, Computer Graphics and Deep Learning with an application domain of Augmented and Virtual Reality.","tags":null,"title":"","type":"authors"},{"authors":["Srinidhi Hegde"],"categories":["Off-road Musings"],"content":"In case you find art and geometry fascinating, here is my collection of some snaps that held my gaze.\n(Subject to Copyright © Srinidhi Hegde 2022.)\n","date":1657325097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657325097,"objectID":"3f5df4e7c6746eb281f3a314d89e79d5","permalink":"https://srihegde.github.io/post/art_album/","publishdate":"2022-07-08T19:04:57-05:00","relpermalink":"/post/art_album/","section":"post","summary":"In case you find art and geometry fascinating, here is my collection of some snaps that held my gaze.\n(Subject to Copyright © Srinidhi Hegde 2022.)","tags":[],"title":"Kaleidoscope","type":"post"},{"authors":["Srinidhi Hegde"],"categories":["Off-road Musings"],"content":"Peering out of the window on a lazy Sunday afternoon I noticed something peculiar about the house across the street. A break of 3000 odd house numbers between my neighbor and my house didn’t add up to me. My neighborhood was a tiny enclosure amidst the crotched bustling highways of Maryland. How in the world could this small patch of land fit all these houses? Revelations and tragedy always strike with an element of surprise. But what is worse? Both struck me simultaneously as I realized that these unaccounted houses were the progeny of my amnesic memory which threw up an incorrect house number for my new residence. However, I don’t blame myself for this. This episode happened while I was settling in a foreign land having stuck to a single house number all my life.\nBut wait, why is this a tragedy, you ask? Well, this was the incorrect address that I typed in while ordering my brand new bicycle (or bike, as Americans call it) from Walmart. Regaining my composure, I first checked if this incorrect address actually existed, so that I can just collect it from there. But remember, it was a tiny neighborhood and I had directed my package to a house that was 3000 houses away and literally nowhere on earth. So I did the next natural thing that a panic-struck customer does. Calling the Walmart support center. A kind woman answered my call and as I poured out my vexation she looked up my order details. After several rounds of information exchange, we figured out that we could not cancel the order as it was an expensive purchase and it was on its way to delivery. “Can the delivery service at least deliver my bike to the updated location?”, I asked. After a doubtful pause and a quick consultation with her supervisor, she announced, “Yes, you can expect the package within 2 days”. And with that, I gave a sigh of relief. But little did I know how far I was from the relief!\nThree days had passed and there was still no sign of my bike and my paranoia was ballooning. Being a faithful paranoid customer, I dialed up the support center again. This time I heard a different male voice from the other side. And this meant I had to revisit my predicament and also the solution which we had arrived at. After hearing my account, the support staff pondered for a while, verified the purchase records, and nonchalantly declared that my order was canceled and the refund was initiated. After hearing this unpleasant news, I hung up the call and immediately rushed to reorder the same bike. But the product had run out of stock! Having surveyed gazillion bikes online, gauging their tire size, off-road riding, price, … (replace with any filter you can think of, yes including gender), I had lost it all. With a heavy heart, I settled for the next best bike on my surveyed bike list. And I promptly ordered this bike online, of course, with the right address this time.\nThe next morning as I got ready to leave for my early morning classes in the university, I saw a huge tattered cardboard box blocking my house entrance. And what’s more? The box was addressed for me and the first thing that clicked me was the new bike that I ordered the day before. Boy was I impressed by Walmart’s supersonic delivery. Since I was getting late for my lecture, controlling my excitement, I shoved the hefty package inside the house and rushed for the classes. To be honest, I could not concentrate on the lecture that day. Biking to campus, swishing past the pedestrians, minting precious time for myself, and thoughts alike kept feeding the butterflies in my stomach. Yes, I know I didn’t care much about its features, for the bike was the cheapest and the most basic one, probably in the entire campus, that could just take me places. In short, I was fixated on the tattered box in my verandah.\nAs the class ended that day, it was probably one of the fastest walks (actually interspersed bursts of sprints) that I had to my home. I unlocked the front door of my house, rushed towards the box picking the nearby scissors. Snip! Snip! I effortlessly tore open the already tattered box. But what do I discover here? This was the bike that I had ordered before ordering my replacement bike that was inferior to my first bike. Did you read too many bikes? Don’t worry. Let us call the first bike (that was canceled) the red bike and the second bike (the replacement) the blue bike and I had received the red bike in the package. Well, this was a confusing situation for me. To aggravate the matters, I woke up to another package the next day, waiting for me. I carefully went over the same exercise of tearing-up-the-tattered-cardboard. What do I find here? Yes, you guessed it right. It was the blue bike (yikes! I have 2 bikes!). That was the moment I was lost somewhere amidst the confusion of joy, awe, anxiety(over returning and refund process), and, anger (directed at Walmart and partly to myself).\nAfter several unsuccessful attempts to bring me to my senses, it took me three weeks to accept what had happened. I had neither received the refund for the red bike (which was anticipated in 8-10 business days) nor did I hear anything from Walmart till then. And I was the unexpected owner of two bikes and I was presented with the choice of red cycle versus the blue cycle. Although it sounds like the iconic red and blue pills of Keanu Reaves’ Matrix. But making a decision here was not that hard. I decided to keep the red one but what to do with the blue bike? Luckily for me, one of my housemates did not have a bike and it took me a little bit of convincing to sell my bike to him (how I marketed my product could be a topic for a separate blog post).\nNow, if you are wondering if this was my “happily ever after” moment, then hold your horses. After a week, almost a month had passed since the refund for my red bike was initiated. It seems like the stars aligned well that day and I got my refund. So now I had two bikes one of which was free! Finally, I was relieved. But in my hindsight, I had a sinister feeling that something was not right. Moreover, one of my friends, after hearing my case, subtly prompted that this seems illegal and I could be up for some trouble. Firing up my researcher instincts, I scoured through the internet to verify if I am the unlucky one. Well to my surprise, not one or two but hundreds of cases like this crop up every year due to logistical goof-ups. To make the matters worse, I also read that some state jurisdictions consider this as theft and a 6th-degree felony leading up to a year’s prison term! Instantly, my relief switched to paranoia. Do I have to return my bike? My favorite red bike? Or even worse, will I be incarcerated? To remedy this situation, I dialed the Walmart service center once again and apologetically explained myself to the support staff. After a momentary pause, which seemed like eons to me, I heard the three golden words - “Just keep it!”. No questions asked. Period. By this time, I was so used to my paranoia that I breathed a sigh of relief anyway. I finally owned a freebie bike legally. How cool is that?\nEven now, just to give my creativity some rest, I recycle this story as icebreakers in conversations during parties, dinners, and even in classrooms! (What’s worse, I can refer to this blog from now on.) My audience generally compliments my honesty and some are even surprised that I called the support center to clarify. But little do they know that this should be credited to my guilt and paranoia. Looking back at this whole episode, I do and don’t blame Walmart for this episode. But I can’t complain either. On the flip side, I always cherish how I came so close to stealing a bike and getting away with it!\nIf you have made it so far, I thank you for your awesomeness.\n","date":1641168297,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168297,"objectID":"d4480befcb866daa788de2b632037ad5","permalink":"https://srihegde.github.io/post/bikesaga/","publishdate":"2022-01-02T19:04:57-05:00","relpermalink":"/post/bikesaga/","section":"post","summary":"Peering out of the window on a lazy Sunday afternoon I noticed something peculiar about the house across the street. A break of 3000 odd house numbers between my neighbor and my house didn’t add up to me.","tags":[],"title":"The \"Bi\"-Cycle Saga","type":"post"},{"authors":["M. Gwilliam","S. Hegde","L Tinubu","A Hanson"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"d5e7a475a49de95dfe3c9d324ea92c8a","permalink":"https://srihegde.github.io/publication/iccv2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/iccv2021/","section":"publication","summary":"Many existing works have made great strides towards reducing racial bias in face recognition. However, most of these methods attempt to rectify bias that manifests in models during training instead of directly addressing a major source of the bias, the dataset itself. Exceptions to this are BUPT-Balancedface/RFW and Fairface, but these works assume that primarily training on a single race or not racially balancing the dataset are inherently disadvantageous. We demonstrate that these assumptions are not necessarily valid. In our experiments, training on only African faces induced less bias than training on a balanced distribution of faces and distributions skewed to include more African faces produced more equitable models. We additionally notice that adding more images of existing identities to a dataset in place of adding new identities can lead to accuracy boosts across racial categories. Our code is available at https://github.com/j-alex-hanson/rethinking-race-face-datasets.","tags":[],"title":"Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets","type":"publication"},{"authors":["A. Khattar","S. Hegde","R. Hebbalaguppe"],"categories":null,"content":"","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"059b347930c2bbdaf20a803bd586576a","permalink":"https://srihegde.github.io/publication/cvprw2021/","publishdate":"2021-06-19T00:00:00Z","relpermalink":"/publication/cvprw2021/","section":"publication","summary":"Multi-task learning (MTL) is a learning paradigm that aims at joint optimization of multiple tasks using a single neural network for better performance and generalization. In practice, MTL rests on the inherent assumption of availability of common datasets with ground truth labels for each of the downstream tasks. However, collecting such a common annotated dataset is laborious for complex computer vision tasks such as the saliency estimation which would require the eye fixation points as the ground truth data. To this end, we propose a novel MTL framework in the absence of common annotated dataset for joint estimation of important downstream tasks in computer vision - object detection and saliency estimation. Unlike many state-of-the-art methods, that rely on common annotated datasets for training, we consider the annotations from different datasets for jointly training different tasks, calling this setting as cross-domain MTL. We adapt MUTAN framework to fuse features from different datasets to learn domain invariant features capturing the relatedness of different tasks. We demonstrate the improvement in the performance and generalizability of our MTL architecture. We also show that the proposed MTL network offers a 13% reduction in memory footprint due to parameter sharing between the related tasks.","tags":[],"title":"Cross-Domain Multi-task Learning for Object Detection and Saliency Estimation","type":"publication"},{"authors":["S. Yalburgi","T. Dash","R. Hebbalaguppe","S. Hegde","A. Srinivasan"],"categories":null,"content":"","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"08737ab22db38567412fd45a2fdd9974","permalink":"https://srihegde.github.io/publication/esann2020/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/publication/esann2020/","section":"publication","summary":"In this paper we introduce Iterative Knowledge Distillation (IKD), the process of successively minimizing models based on the Knowledge Distillation (KD) approach in Hinton et al. We study two variations of IKD, called parental- and ancestral- training. Both use a single-teacher, and result in a single-student model: the differences arise from which model is used as a teacher. Our results provide support for the utility of the IKD procedure, in the form of increased model compression, without significant losses in predictive accuracy. An important task in IKD is choosing the right model(s) to act as a teacher for a subsequent iteration. Across the variations of IKD studied, our results suggest that the most recent model constructed (parental-training) is the best single teacher for the model in the next iteration. This result suggests that training in IKD can proceed without requiring us to keep all models in the sequence.","tags":[],"title":"An Empirical Study of Iterative Knowledge Distillation for Neural Network Compression","type":"publication"},{"authors":["S. Hegde","R. Prasad","R. Hebbalaguppe","V. Kumar"],"categories":null,"content":"","date":1588550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588550400,"objectID":"10846a107312909bfeea26f6a647a5e6","permalink":"https://srihegde.github.io/publication/icassp2020/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/publication/icassp2020/","section":"publication","summary":"The holy grail in deep neural network research is porting the memory- and computation-intensive network models on embedded platforms with a minimal compromise in model accuracy. To this end, we propose a novel approach, termed as Variational Student, where we reap the benefits of compressibility of the knowledge distillation (KD) framework, and sparsity inducing abilities of variational inference (VI) techniques. Essentially, we build a sparse student network, whose sparsity is induced by the variational parameters found via optimizing a loss function based on VI, leveraging the knowledge learnt by an accurate but complex pre-trained teacher network. Further, for sparsity enhancement, we also employ a Block Sparse Regularizer on a concatenated tensor of teacher and student network weights. We demonstrate that the marriage of KD and the VI techniques inherits compression properties from the KD framework, and enhances levels of sparsity from the VI approach, with minimal compromise in the model accuracy. We benchmark our results on LeNet MLP and VGGNet (CNN) and illustrate a memory footprint reduction of 64x and 213x on these MLP and CNN variants, respectively, without a need to retrain the teacher network. Furthermore, in the low data regime, we observed that our method outperforms state-of-the-art Bayesian techniques in terms of accuracy.","tags":[],"title":"Variational Student: Learning Compact and Sparser Networks in Knowledge Distillation Framework","type":"publication"},{"authors":["S. Hegde","J. Maurya","A. Kalkar","R. Hebbalaguppe"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"33d131c67ca378e17eb6fe1371966a3e","permalink":"https://srihegde.github.io/publication/wacv2020/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/wacv2020/","section":"publication","summary":"","tags":[],"title":"SmartOverlays: A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces","type":"publication"},{"authors":[],"categories":[],"content":"Compressing the memory-intensive DNN models with a minimal compromise in model accuracy using variational methods and knowledge distillation. Another part of projects involves proposing theoretical guarantees on the knowledge distillation models for efficient neural architecture search.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"831d1ad0dbf77e63346af26bafe8891e","permalink":"https://srihegde.github.io/project/nncomp/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/nncomp/","section":"project","summary":"Compressing deep neural network models for memory constrained devices","tags":["neural networks","deep learning","optimization","variational inference"],"title":"Deep Neural Network Compression","type":"project"},{"authors":[],"categories":[],"content":"Automating the animation through learning and transferring motion cues from the real RGB-D videos to virtual 3D meshes. We use graph based structure correspondences to map the motion between the two 3D entities such as point cloud and 3D meshes.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"7f0371b66ae970b1302cb3fa6604babd","permalink":"https://srihegde.github.io/arxiv_projects/animation/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/arxiv_projects/animation/","section":"arxiv_projects","summary":"Automating animation through motion transfer from real scenes.","tags":["computer graphics","computer vision","animation","machine learning"],"title":"Reverse VooDoo","type":"arxiv_projects"},{"authors":[],"categories":[],"content":"A novel method for the placement of labels corresponding to objects of interest in images/videos/live feeds that is non-intrusive, relevant and temporally coherent. We employ different techniques ranging from search space optimization to visual saliency based neural network framework.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"2cd5acc906258452e0e9aa258e8d59ba","permalink":"https://srihegde.github.io/project/smartov/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/project/smartov/","section":"project","summary":"A non-intrusive, relevant and temporally coherent overlay placement technique","tags":["situated visualization","augmented reality","computer vision","visual saliency"],"title":"SmartOverlays","type":"project"},{"authors":["G. Garg","S. Hegde","R. Perla","V. Jain","L. Vig","R. Hebbalaguppe"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"15188d8fad6a9fe827f1771bd7ec8392","permalink":"https://srihegde.github.io/publication/eccv2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/eccv2018/","section":"publication","summary":"Hand gestures form a natural way of interaction on Head-Mounted Devices (HMDs) and smartphones. HMDs such as the Microsoft HoloLens and ARCore/ARKit platform enabled smartphones are expensive and are equipped with powerful processors and sensors such as multiple cameras, depth and IR sensors to process hand gestures. To enable mass market reach via inexpensive Augmented Reality (AR) headsets without built-in depth or IR sensors, we propose a real-time, in-air gestural framework that works on monocular RGB input, termed, DrawInAir.DrawInAir uses fingertip for writing in air analogous to a pen on paper. The major challenge in training egocentric gesture recognition models is in obtaining sufficient labeled data for end-to-end learning. Thus, we design a cascade of networks, consisting of a CNN with differentiable spatial to numerical transform (DSNT) layer, for fingertip regression, followed by a Bidirectional Long Short-Term Memory(Bi-LSTM), for a real-time pointing hand gesture classification. We highlight how a model,that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better overend-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model. We showthat the framework takes 1.73s to run end-to-end and has a low memory footprint of 14MB while achieving an accuracy of 88.0% on egocentric video dataset.","tags":[],"title":"DrawInAir: A Lightweight Gestural Interface Based on Fingertip Regression","type":"publication"},{"authors":["N. Rakholia","S. Hegde","R. Hebbalaguppe"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"4dc7d3c2966a164272530172b993be87","permalink":"https://srihegde.github.io/publication/icip2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/icip2018/","section":"publication","summary":"Textual overlays/labels add contextual information in Augmented Reality (AR) applications. The spatial placement of labels is a challenging task due to constraints that labels (i) are not occluding the object/scene of interest, and, (ii) are optimally placed for better interpretation of scene. To this end, we present a novel method for optimal placement of labels for AR. We formulate this method by an objective function that minimizes both occlusion with visually salient regions in scenes of interest, and the temporal jitter for facilitating coherence in real-time AR applications. The main focus of proposed algorithm is real-time label placement on low-end android phones/tablets. The sophisticated state-of-the-art algorithms for optimal positioning of textual label work only on the images and are often inefficient for real-time performance on those devices. We demonstrate the efficiency of our method by porting the algorithm on a smart-phone/tablet. Further, we capture objective and subjective metrics to determine the efficacy of the method; objective metrics include computation time taken for determining the label location and Label Occlusion over Saliency (LOS) score over salient regions in the scene. Subjective metrics include position, temporal coherence in the overlay, color and responsiveness.","tags":[],"title":"Where To Place: A Real-Time Visual Saliency Based Label Placement for Augmented Reality Applications","type":"publication"},{"authors":[],"categories":[],"content":"Hand gesture classification through fingertip coordinate regression in a temporal model for touch-less interactions in AR. We highlight how a model, that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better over end-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model.\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"95083189b28bb1a09399d31cc3c5e338","permalink":"https://srihegde.github.io/project/drawinair/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/project/drawinair/","section":"project","summary":"A hand gesture based in-air touch-less interaction technique for AR applications","tags":["hand gestures","computer vision","augmented reality","deep learning"],"title":"Draw In Air","type":"project"},{"authors":[],"categories":[],"content":"Employing CNNs for an end-to-end reconstruction of the indoor scenes through camera relocalization, through PoseNet, and depth estimation, through multi-scale fully convolutional network, from a single RGB image during inference and registering the 3D reconstructed patches through iterative closest point algorithm. A portion of the dataset collected during the project is also released.\n","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"f109aed8fe02906bc2c731735cb883dc","permalink":"https://srihegde.github.io/project/btp/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/project/btp/","section":"project","summary":"Developing an end to end pipeline for automatic generation of 3D models of indoor scenes.","tags":["deep learning","3D reconstruction","camera relocalization","computer vision"],"title":"Robust 3D Reconstruction of Indoor Scenes using Deep Learning","type":"project"},{"authors":["S. Hegde","R. Perla","R. Hebbalaguppe","E. Hassan"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"03671fcde735b5d8d799d0ebf19df59e","permalink":"https://srihegde.github.io/publication/ismar2016/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/ismar2016/","section":"publication","summary":"The existing, sophisticated AR gadgets in the market today are mostly exorbitantly priced. This limits their usage for the upcoming academic research institutes and also their reach to the mass market in general. Among the most popular and frugal head mounts, Google Cardboard (GC) and Wearality are video-see-through devices that can provide immersible AR and VR experiences with a smartphone. Stereo-rendering of camera feed and overlaid information on smartphone helps us experience AR with GC. These frugal devices have limited user-input capability, allowing user interactions with GC such as head tilting, magnetic trigger and conductive lever. Our paper proposes a reliable and intuitive gesture based interaction technique for these frugal devices. The hand gesture recognition employs the Gaussian Mixture Models (GMM)based on human skin pixels and tracks segmented foreground using optical flow to detect hand swipe direction for triggering a relevant event. Real-time performance is achieved by implementing the hand gesture recognition module on a smartphone and thus reducing the latency. We augment real-time hand gestures as new GC’s interface with its evaluation done in terms of subjective metrics and with the available user interactions in GC.","tags":[],"title":"GestAR: Real Time Gesture Interaction for AR with Egocentric View","type":"publication"},{"authors":[],"categories":[],"content":"This work develops a distributed fault tolerant area coverage algorithm, resulting in quick detection of the faulty agent under limited communication constraints and redistributes the area without conflicts.\n","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"e500c976dc3bf8d2d7047c9d11fe6935","permalink":"https://srihegde.github.io/project/mas/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/project/mas/","section":"project","summary":"Analysed Multi-Agent area coverage problem for surveillance purposes.","tags":["multi-agent systems","area coverage","scheduling","robotics"],"title":"Distributed Fault Tolerant Multi-Robot Area Coverage Under Limited Communication Ranges","type":"project"},{"authors":[],"categories":[],"content":"Realistic models of vegetations are very essential piece of immersive virtual environment simulations. We develop a novel technique to convert a single captured image of a vegetation to a 3D model using L-Systems, a context-free grammar that we adapt to procedurally model vegetation. We also propose a pipeline that is semi-automated with manual interventions for accurate identification of tree branches and trunk.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7ba1e13a3b867f0d96e2ccdc01f65e5d","permalink":"https://srihegde.github.io/project/lsys/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/lsys/","section":"project","summary":"Procedural modelling of vegetation using context-free grammar","tags":["Computer Graphics","3D Modelling","Context-free Grammar"],"title":"Modelling Vegetation with L-Systems Using an Image","type":"project"},{"authors":[],"categories":[],"content":"Estimating GPS location of a single RGB image of outdoor environment by, firstly, GPS coordinate retrieval from image classification and secondly, fine tuning the location estimate using structure from motion and and position triangulation. This application was interfaced by an Android mobile application.\n","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"8f2951175dba63774f20f2fe2c3834cd","permalink":"https://srihegde.github.io/arxiv_projects/visloc/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/arxiv_projects/visloc/","section":"arxiv_projects","summary":"Outdoor location estimation in IIIT-Delhi campus through computer vision","tags":["computer vision","camera relocalization","machine learning"],"title":"Vision Based Outdoor Localization","type":"arxiv_projects"},{"authors":[],"categories":[],"content":"We present an interactive sketching interface for quick and easy designing of freeform 3D models using OpenGL and CGAL libraries in C++. The mesh construction employs Shewchuk\u0026rsquo;s algorithm (which is based on Delaunay Triangulation). The freehand interaction and mesh construction is perfomed in real-time.\n","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"58c495af465971635509f3843e6a5402","permalink":"https://srihegde.github.io/project/sketch23d/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/project/sketch23d/","section":"project","summary":"A sketching interface for rapid designing of freeform 3D models from 2D sketches","tags":["Computer Graphics","3D Modelling"],"title":"Sketch23D","type":"project"},{"authors":[],"categories":[],"content":"This project proposes to build a virtual smart campus infrastructure. A 3D interactive and immersive virtual/mixed reality environment will be designed to support geospatial services including smart navigation and telepresence. Two key aspects of the system are 3D modeling and rendering.\n","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"44ec17be3a14c58dca91019dafd2a177","permalink":"https://srihegde.github.io/project/vcp/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/project/vcp/","section":"project","summary":"Creating a virtual walk-through of IIIT-Delhi campus","tags":["computer graphics","immersive rendering","virtual reality"],"title":"Virtual Campus Project","type":"project"},{"authors":null,"categories":null,"content":"Developed a web application, that takes input from the user about his/her preferences about the specification of the machine which include - operating system, main memory space, disk(storage) space, number of cores. Then we return the IP of the machine (Virtual Machine) assigned according to the mentioned preferences, for a particular amount of time. We use KVM for VM management.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d2e9ca8e3a91248d536db1ea130e8a0","permalink":"https://srihegde.github.io/arxiv_projects/vcloud/vcloud/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/arxiv_projects/vcloud/vcloud/","section":"arxiv_projects","summary":"Cloud based Virtual Machine management system using KVM.","tags":["System Management"],"title":"Creating Cloud of Local Servers","type":"arxiv_projects"},{"authors":null,"categories":null,"content":" 17th July 2023: Our work \u0026ldquo;Diffusion Models Beat GANs on Image Classification\u0026rdquo; published on Arxiv. Check it out here.\n21st May 2023: Graduated from UMD with masters in Computer Science!\n18th Oct 2021: My co-authored work \u0026ldquo;Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets\u0026rdquo; won the best paper running up award at ICCV (HTCV \u0026lsquo;21)! Congrats to all the co-authors.\n1st Aug 2021: My co-authored work \u0026ldquo;Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets\u0026rdquo; accepted at ICCV (HTCV \u0026lsquo;21) as a oral paper. My first publication as a UMD student!\n25th May 2021: I have started my grad school at University of Maryland, College Park.\n13th Apr 2021: My co-authored work \u0026ldquo;Cross-Domain Multi-task Learning for Object Detection and Saliency Estimation\u0026rdquo; accepted at CVPR (CLVISION \u0026lsquo;21) as a poster.\n14th Feb 2021: Received TCS Citation Award for the second time for outstanding contributions to the organization through publications.\n20th July 2020: Invited as a reviewer for IEEE Transactions on Circuits and Systems for Video Technology (TCSVT, impact factor: 3.599). My first journal reviewing experience!\n21st Mar 2020: TreasAR Hunt - an AR based treasure hunt - organized for Re.Fresh 2020 at TCS Innovation Labs New Delhi. The source code is now available. Check it out!\n24th Jan 2020: Variational Student: Our work on neural network compression through sparsification in Knowledge Distillation framework got accepted as an \u0026ldquo;oral presentation\u0026rdquo; at ICASSP 2020 to be held at Barcelona, Spain.\n24th Jan 2020: IKD: Our work on empirical analysis of iterative knowledge distillation methods got accepted at ESANN 2020 to be held at Bruges, Belgium.\n17th Sep 2019: SmartOverlays: Our work on situated visualization in AR and video application got accepted at WACV 2020 to be held at Aspen, Colorado.\n6th Jul 2019: Attended EEML Summer School 2019, Bucharest, Romania. My experience was awesome!\n15th March 2019: Selected for EEML Summer School 2019. Travelling to Bucharest, Romania in July.\n21st Jan 2019: TCS-PanIIT Conclave 2019- Hackathon Update: Mentored teams finished at 3rd and 4th positions in the event. Kudos to the team members!\n14th Jan 2019: Selected as mentor for PanIIT Hackathon at TCS-PanIIT Conclave 2019 organized by TCS and IIT Delhi.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9eb50f9088083bebcb7e4cf99e22b9ed","permalink":"https://srihegde.github.io/news/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"17th July 2023: Our work \u0026ldquo;Diffusion Models Beat GANs on Image Classification\u0026rdquo; published on Arxiv. Check it out here.\n21st May 2023: Graduated from UMD with masters in Computer Science!","tags":null,"title":"Events Archive","type":"page"},{"authors":null,"categories":null,"content":"In this project we developed an IPv4 packet generator to simulate any network traffic the user wants. We interfaced this through a web application to provide user the provision for customizing source and destination IPs and ports,the protocol to be used and amount of that data each of the packets carry. We analyzed the most used protocols and identifying clogging points in network.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b3b0ba6dce2f7c30059ef4b7f68982f5","permalink":"https://srihegde.github.io/arxiv_projects/netpack/netpack/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/arxiv_projects/netpack/netpack/","section":"arxiv_projects","summary":"Network traffic analysis and IPv4 packet generation for custom simulation.","tags":["System Management","Network Management"],"title":"Network Traffic Generator and Packet Analyzer","type":"arxiv_projects"}]