<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Srinidhi Hegde</title>
    <link>https://srihegde.github.io/</link>
      <atom:link href="https://srihegde.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Srinidhi Hegde</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Some rights reserved ©2024. Srinidhi Hegde</copyright><lastBuildDate>Sat, 01 Jun 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://srihegde.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Srinidhi Hegde</title>
      <link>https://srihegde.github.io/</link>
    </image>
    
    <item>
      <title>NARViS</title>
      <link>https://srihegde.github.io/project/narvis/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/narvis/</guid>
      <description>&lt;p&gt;Exploring scientific datasets with billions of samples in real-time visualization presents a challenge - balancing high-fidelity rendering with speed. This work introduces a novel method that uses the neural deferred rendering framework to visualize large-scale scientific point cloud data. Our approach augments a real-time high-quality point-based rendering pipeline with neural post-processing, enabling real-time manipulation of the scene elements. The modularity of the proposed framework allows customization of different stages of the rendering pipeline making it applicable to various post-processing effects. Furthermore, we also show that we can achieve a high-quality visualization with the desired post-processing effects without using the full resolution of the original point cloud. Our method prioritizes speed and scalability, making it ideal for interactive exploration and analysis of large-scale scientific datasets. Furthermore, we showcase our approach’s effectiveness on various complex scientific datasets, demonstrating real-time rendering with user-defined styles and improved visual clarity, ultimately fostering a more efficient and engaging scientific visualization experience.&lt;/p&gt;
&lt;p&gt;Pre-print and code to be released soon. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Never Miss a Deadline Again</title>
      <link>https://srihegde.github.io/post/dlbot/</link>
      <pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/post/dlbot/</guid>
      <description>&lt;p&gt;Ever wished you could stay on top of conference deadlines without the constant calendar checking? Well, fret no more! Two missed deadlines later (yikes!), I knew I needed a better system for keeping track of them. Enter automation!&lt;/p&gt;
&lt;p&gt;Today, in this article I&amp;rsquo;ll talk about the world of Twitter bots and how to build our very own conference deadline tracker bot using Python. Although there are handful of bot accounts on Twitter keeping track of conference deadlines, none of them are customized to my conference requirements in computer graphics, 3D vision, and visualization. Hence we will build our little bot friend who will scour the depths of our (shallow) data and tweet out those all-important deadlines, keeping us informed and stress-free.  Let&amp;rsquo;s get started!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shameless plug:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;🔥Please do me a favor and follow &lt;a href=&#34;https://twitter.com/confclock&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@confclock&lt;/a&gt; on twitter for daily updates on graphics, vision, and visualization conference deadlines🔥&lt;/p&gt;
&lt;h3 id=&#34;prerequisites-what-youll-need&#34;&gt;Prerequisites: What You&amp;rsquo;ll Need&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A free Twitter developer account (we&amp;rsquo;ll need to chat with Twitter through code)&lt;/li&gt;
&lt;li&gt;A little Python know-how (don&amp;rsquo;t worry, it&amp;rsquo;s beginner-friendly!)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-1-sign-up-and-get-access&#34;&gt;Step 1: Sign Up and Get Access&lt;/h3&gt;
&lt;p&gt;First things first, head over to Twitter&amp;rsquo;s developer portal and snag yourself a free developer account. You might need to write a quick blurb about why you want to use their fancy Twitter APIs (think of it as your bot&amp;rsquo;s resume). Congratulations(if you got the access)! You can pull upto 1500 posts per month from Twitter if it is a free developer account.&lt;/p&gt;
&lt;h3 id=&#34;step-2-project-time&#34;&gt;Step 2: Project Time!&lt;/h3&gt;
&lt;p&gt;Once you&amp;rsquo;re in, you&amp;rsquo;ll see a new project with a random name. Give it a snazzy name that reflects your bot&amp;rsquo;s purpose (For e.g., &amp;ldquo;GraphViz DL&amp;rdquo; for our case or something drippy - &amp;ldquo;Deadline Slayer 3000&amp;rdquo;). There are two things here a project and an app. You can have projects in your account. Also you can have multiple apps under a project and each app servers a different purpose.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;im1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-3-building-your-bot-account&#34;&gt;Step 3: Building Your Bot Account&lt;/h3&gt;
&lt;p&gt;Here comes the fun part: creating your bot account! You can refer to &lt;a href=&#34;https://tweetdelete.net/resources/how-to-create-a-second-twitter-account-a-quick-start-guide/#How_To_Create_a_Second_Twitter_Account_With_Same_Email&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this nifty article&lt;/a&gt; that talks about creating a new twitter account by &amp;ldquo;hacking&amp;rdquo; the twitter login checker. Unfortunately, if none of that works for you then you&amp;rsquo;ll need a separate email address or phone number. So dust off an old email or grab a temporary phone number service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;im2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-4-talk-twitter-talk-authentication&#34;&gt;Step 4: Talk Twitter Talk (Authentication!)&lt;/h3&gt;
&lt;p&gt;Now, we need to get our bot authorized to use Twitter&amp;rsquo;s superpowers. This involves generating some special codes or tokens and saving them securely. Think of them as secret handshakes that unlock Twitter&amp;rsquo;s features for your bot.&lt;/p&gt;
&lt;p&gt;For this go to the Settings -&amp;gt; User Authentication Settings and update the app permissions, type of app, and the app info sections as per your requirements. I have used the following settings for my bot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;im3.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;im4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now to generate and save the tokens, go to the Keys and Tokens tab in your Twitter developer account. You&amp;rsquo;ll see five keys here: API key, API secret key, Bearer token, Access token, and Access token secret. These are the secret handshakes we need to talk to Twitter (which uses &lt;a href=&#34;https://developer.twitter.com/en/docs/authentication/oauth-1-0a/obtaining-user-access-tokens&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3-legged OAuth Flow&lt;/a&gt; behind the scenes).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;im5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-5-python-power-writing-the-script&#34;&gt;Step 5: Python Power! Writing the Script&lt;/h3&gt;
&lt;p&gt;This is where the Python magic happens! We&amp;rsquo;ll write a script that uses a cool library called &amp;ldquo;&lt;a href=&#34;https://github.com/tweepy/tweepy/tree/v4.14.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweepy&lt;/a&gt;&amp;rdquo; to chat with Twitter (with v2 endpoint APIs) and send out those deadline tweets. We&amp;rsquo;ll also set up a system to read conference information from a file (like a CSV spreadsheet).&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a simple script to get you started:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tweepy as tpy
import os
import csv
from datetime import datetime
from typing import Tuple, Dict
from dotenv import load_dotenv

# Authentication tokens
root_path = &#39;./&#39;
load_dotenv(os.path.join(root_path, &#39;.env&#39;))

consumer_key = os.getenv(&amp;quot;CONSUMER_KEY&amp;quot;)
consumer_secret = os.getenv(&amp;quot;CONSUMER_SECRET&amp;quot;)
access_token = os.getenv(&amp;quot;ACCESS_TOKEN&amp;quot;)
access_token_secret = os.getenv(&amp;quot;ACCESS_TOKEN_SECRET&amp;quot;)

# Authenticate to Twitter
client = tpy.Client(consumer_key=consumer_key,
                    consumer_secret=consumer_secret,
                    access_token=access_token,
                    access_token_secret=access_token_secret)

# Tweet the message
client.create_tweet(&amp;quot;Hello, Twitter!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Don&amp;rsquo;t be a noob and store the keys in the script itself. Instead, use a &lt;code&gt;.env&lt;/code&gt; file to store the keys (eg. COMSUMER_KEY=xxxx&amp;hellip;xx, etc.) and load them into your script using &lt;code&gt;load_dotenv&lt;/code&gt; module. This way, your keys stay secure and your code is modular.&lt;/p&gt;
&lt;h3 id=&#34;step-6-tweeting-from-the-bot-account&#34;&gt;Step 6: Tweeting from the Bot Account&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s the tricky bit. We&amp;rsquo;ve written a script that can tweet, but it&amp;rsquo;s currently tweeting from your main account. We need to link it to your bot account so the deadlines get posted there. There are a couple of ways to do this, and we&amp;rsquo;ll be using a method with a fancy name: OAuth 1.0a User Context.&lt;/p&gt;
&lt;p&gt;This involves a bit of back-and-forth between your code and Twitter to get the necessary permissions. Long story short, you&amp;rsquo;ll need to generate an authentication link for your bot account to access the app&amp;rsquo;s functionality (remember app is in your main account), click on the link, and access the secret tokens of your bot account to be used in the app to tweet on bot&amp;rsquo;s behalf.  Don&amp;rsquo;t worry, the instructions will guide you through it step-by-step.&lt;/p&gt;
&lt;p&gt;Firstly, we need to generate the an authentication link for the bot account to access the app&amp;rsquo;s functionality. This can be done by using the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;consumer_key = os.getenv(&amp;quot;CONSUMER_KEY&amp;quot;)
consumer_secret = os.getenv(&amp;quot;CONSUMER_SECRET&amp;quot;)

oauth1_user_handler = tpy.OAuth1UserHandler(
  consumer_key=consumer_key, consumer_secret=consumer_secret,
  callback=&#39;https://twitter.com&#39;)

# The following link will ask you to authorize the app to use your account.
# Authorize and then you will be redirected to twitter.com with an oauth_verifier 
# field in the URL. Pass that in the prompt of the next cell.
print(oauth1_user_handler.get_authorization_url(signin_with_twitter=True))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here consumer key and secret are from the main account. This script spits the authentication link (which will look something like - &lt;em&gt;&lt;a href=&#34;https://twitter.com/home?oauth_token=%3csome_oauth_token%3e&amp;amp;oauth_verifier=%3csome_oauth_verifier%3e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/home?oauth_token=&amp;lt;some_oauth_token&amp;gt;&amp;amp;oauth_verifier=&amp;lt;some_oauth_verifier&amp;gt;&lt;/a&gt;&lt;/em&gt;). Click on the link and authorize the app to use your account. You will be redirected to twitter.com with an &lt;code&gt;oauth_verifier&lt;/code&gt; field in the URL. Copy that and pass it in the next cell below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;verifier = input(&#39;Verifier: &#39;)
# Save the following tokens securely in .env file
bot1_access_token, bot1_access_token_secret = oauth1_user_handler.get_access_token(verifier)

# Authenticate to Twitter with the bot account
client = tpy.Client(consumer_key=consumer_key,
                    consumer_secret=consumer_secret,
                    access_token=bot1_access_token,
                    access_token_secret=bot1_access_token_secret)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have the bot account&amp;rsquo;s access tokens. Save them securely in the &lt;code&gt;.env&lt;/code&gt; file. Now you can tweet from the bot account using tweepy&amp;rsquo;s &lt;code&gt;create_tweet()&lt;/code&gt; function. Also note that this approach requires too much manual intervention. You can automate this setup by creating some sort of web application that would parse the url and retrieve the oauth_verifier. But I did this because this is a one-time process and we are saving the required tokens of the bot account.&lt;/p&gt;
&lt;h3 id=&#34;step-7-feeding-the-beast-conference-deadline-data&#34;&gt;Step 7: Feeding the Beast (Conference Deadline Data)&lt;/h3&gt;
&lt;p&gt;Now that the tweeting part is figured out, we need to tell our bot what to tweet about! Create a file (like a CSV) containing conference names and deadlines. This will be the bot&amp;rsquo;s treasure trove of information.&lt;/p&gt;
&lt;p&gt;I am storing my data in &amp;lt;conference_name, conference_deadline&amp;gt; format.The Python script will read this file, transform it into a tweet-worthy message, and send it out to the Twitterverse. Here&amp;rsquo;s a simple example of how you can read the data from a CSV file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def extract_msg(msg_dict: Dict, ulimit: int=151) -&amp;gt; Tuple[str, Dict]:
    msg = &#39;&#39;
    delkeys = []
    for cname, cinfo in msg_dict.items():
        if cinfo[&#39;days&#39;] == 0:
            msg += f&amp;quot;{cname}: Today! Good Luck!\n&amp;quot;
        elif cinfo[&#39;days&#39;] &amp;gt; 0 and cinfo[&#39;days&#39;] &amp;lt; ulimit:
            msg += f&amp;quot;{cname}: {cinfo[&#39;days&#39;]} days\n&amp;quot;
        elif cinfo[&#39;days&#39;] &amp;lt; 0:
            delkeys.append(cname)

    for k in delkeys:
        del msg_dict[k]

    return msg, msg_dict

def get_deadlines_msg(csvfile: str) -&amp;gt; str:
    with open(csvfile, newline=&#39;&#39;) as f:
        reader = csv.reader(f)
        deadlines = list(reader)

    date_frmt = &#39;%d-%m-%Y&#39;
    msg_dict = {}
    for dl in deadlines:
        cname = dl[0].strip()
        date = dl[1].strip()
        today = datetime.now().strftime(date_frmt)
        days_left = datetime.strptime(date, date_frmt) - datetime.strptime(today, date_frmt)
        msg_dict[cname] = {&#39;date&#39;: date, &#39;days&#39;:days_left.days}

    msg_dict = {k: v for k, v in sorted(msg_dict.items(), key=lambda item: item[1][&#39;days&#39;])}
    msg, msg_dict = extract_msg(msg_dict)

    # Remove the passed deadline from the csv file
    with open(csvfile, &#39;w&#39;, newline=&#39;&#39;) as f:
        writer = csv.writer(f)
        for cname, days_left in msg_dict.items():
            date = days_left[&#39;date&#39;]
            writer.writerow([cname, date])

    return msg

csvfile = &#39;deadlines.csv&#39;
msg = get_deadlines_msg(csvfile)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-8-scheduling-the-tweets&#34;&gt;Step 8: Scheduling the Tweets&lt;/h3&gt;
&lt;p&gt;We don&amp;rsquo;t want to babysit our bot, so let&amp;rsquo;s schedule it to tweet automatically every day. We&amp;rsquo;ll be using a cloud platform called &lt;a href=&#34;https://www.pythonanywhere.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PythonAnywhere&lt;/a&gt; to host our Python application. They offer a free plan that&amp;rsquo;s perfect for this.&lt;/p&gt;
&lt;p&gt;Firstly, create an account on this site and setup the environment on the server. Free account gives you enough space and compute time to run this app couple of times a day. For this we will compile all the above code into a python script, called &lt;code&gt;app.py&lt;/code&gt;, and upload it to the PythonAnywhere server along with &lt;code&gt;.env&lt;/code&gt; and our CSV data file.&lt;/p&gt;
&lt;p&gt;Next, create a virtual environment and install the required packages. You can do this by running the following commands in the bash console on the server:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkvirtualenv dlbot --python=/usr/bin/python3.10 --seeder=pip
workon dlbot
pip install tweepy python-dotenv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s schedule a task in the Tasks tab that runs our &lt;code&gt;app.py&lt;/code&gt; script exactly once at a fixed time of the day. You can do this by adding a new task with the following task command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;workon dlbot; python /&amp;lt;your path to project folder&amp;gt;/app.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s it - your bot is now ready to tweet out those all-important deadlines every day! You can check the logs to see if the tweets are being sent out as expected. If you want to test the bot, you can run the script manually by setting a close enough time to run the script.&lt;/p&gt;
&lt;h3 id=&#34;step-9--bonus-level----automating-deadline-gathering&#34;&gt;Step 9: 🔥🔥 Bonus Level 🔥🔥 - Automating Deadline Gathering&lt;/h3&gt;
&lt;p&gt;(For the super-ambitious coders) Here is a challenge! Imagine a world where your bot finds conference deadlines all by itself! We can explore using a fancy Large Language Models (LLMs) to scrape conference websites and extract deadlines.  This is some next-level bot building!&lt;/p&gt;
&lt;p&gt;Hint: You can use tools like Tavily (&lt;a href=&#34;https://tavily.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tavily.com&lt;/a&gt;) which run over ChatGPT-4.0 APIs to produce detailed research reports given a task. You can prompt engineer the model to extract deadlines from the conference websites effectively.&lt;/p&gt;
&lt;p&gt;If you made it till here, you are awesome! Now check out (and follow 🙏🙏🙏) &lt;a href=&#34;https://twitter.com/confclock&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@confclock&lt;/a&gt; on twitter and see what it can do.&lt;/p&gt;
&lt;p&gt;Peace out! 🤖🚀&lt;/p&gt;
&lt;h3 id=&#34;resources&#34;&gt;Resources:&lt;/h3&gt;
&lt;p&gt;Now if you are thinking about where to start, do not worry. I have got you covered! Here is the link to my &lt;a href=&#34;&#34;&gt;GitHub repository&lt;/a&gt; where you can find all the code used in this blog (and more).&lt;/p&gt;
&lt;p&gt;The world of Twitter bots can be a bit overwhelming, so here are some helpful resources to guide you on your journey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.twitter.com/en/docs/tutorials/how-to-create-a-twitter-bot-with-twitter-api-v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;While a tad outdated, the core concepts are still useful&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://devcommunity.x.com/t/yesterday-everything-was-fine-now-you-currently-have-access-to-a-subset-of-twitter-api-v2-endpoints-and-limited-v1-1-endpoints-e-g-media-post-oauth-only/196198/29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Useful discussion 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://devcommunity.x.com/t/using-one-twitter-developer-account-for-multiple-bot-accounts/204068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Useful discussion 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.pythonanywhere.com/pages/ScheduledTasks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Schedule tasks on PythonAnywhere&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/fHHDM2-If9g?si=l3kg71KNgZPy6xA4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With a little dedication and these handy steps, you&amp;rsquo;ll be well on your way to building your very own conference deadline tracking Twitter bot. No more scrambling to meet deadlines – your bot will be your hero!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaleidoscope</title>
      <link>https://srihegde.github.io/post/art_album/</link>
      <pubDate>Fri, 08 Jul 2022 19:04:57 -0500</pubDate>
      <guid>https://srihegde.github.io/post/art_album/</guid>
      <description>&lt;p&gt;In case you find art and geometry fascinating, &lt;a href=&#34;https://photos.app.goo.gl/TAywH8mpeh2vMkcYA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; is my collection of some snaps that held my gaze.&lt;/p&gt;
&lt;p&gt;(Subject to Copyright © Srinidhi Hegde 2022.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The &#34;Bi&#34;-Cycle Saga</title>
      <link>https://srihegde.github.io/post/bikesaga/</link>
      <pubDate>Sun, 02 Jan 2022 19:04:57 -0500</pubDate>
      <guid>https://srihegde.github.io/post/bikesaga/</guid>
      <description>&lt;p&gt;Peering out of the window on a lazy Sunday afternoon I noticed something peculiar about the house across the street. A break of 3000 odd house numbers between my neighbor and my house didn’t add up to me. My neighborhood was a tiny enclosure amidst the crotched bustling highways of Maryland. How in the world could this small patch of land fit all these houses? Revelations and tragedy always strike with an element of surprise. But what is worse? Both struck me simultaneously as I realized that these unaccounted houses were the progeny of my amnesic memory which threw up an incorrect house number for my new residence. However, I don’t blame myself for this. This episode happened while I was settling in a foreign land having stuck to a single house number all my life.&lt;/p&gt;
&lt;p&gt;But wait, why is this a tragedy, you ask? Well, this was the incorrect address that I typed in while ordering my brand new bicycle (or bike, as Americans call it) from Walmart. Regaining my composure, I first checked if this incorrect address actually existed, so that I can just collect it from there. But remember, it was a tiny neighborhood and I had directed my package to a house that was 3000 houses away and literally nowhere on earth. So I did the next natural thing that a panic-struck customer does. Calling the Walmart support center. A kind woman answered my call and as I poured out my vexation she looked up my order details. After several rounds of information exchange, we figured out that we could not cancel the order as it was an expensive purchase and it was on its way to delivery. “Can the delivery service at least deliver my bike to the updated location?”, I asked. After a doubtful pause and a quick consultation with her supervisor, she announced, “Yes, you can expect the package within 2 days”. And with that, I gave a sigh of relief. But little did I know how far I was from the relief!&lt;/p&gt;
&lt;p&gt;Three days had passed and there was still no sign of my bike and my paranoia was ballooning. Being a faithful paranoid customer, I dialed up the support center again. This time I heard a different male voice from the other side. And this meant I had to revisit my predicament and also the solution which we had arrived at. After hearing my account, the support staff pondered for a while, verified the purchase records, and nonchalantly declared that my order was canceled and the refund was initiated. After hearing this unpleasant news, I hung up the call and immediately rushed to reorder the same bike. But the product had run out of stock! Having surveyed gazillion bikes online, gauging their tire size, off-road riding, price, … (replace with any filter you can think of, yes including gender), I had lost it all. With a heavy heart, I settled for the next best bike on my surveyed bike list. And I promptly ordered this bike online, of course, with the right address this time.&lt;/p&gt;
&lt;p&gt;The next morning as I got ready to leave for my early morning classes in the university, I saw a huge tattered cardboard box blocking my house entrance. And what’s more? The box was addressed for me and the first thing that clicked me was the new bike that I ordered the day before. Boy was I impressed by Walmart’s supersonic delivery. Since I was getting late for my lecture, controlling my excitement, I shoved the hefty package inside the house and rushed for the classes. To be honest, I could not concentrate on the lecture that day. Biking to campus, swishing past the pedestrians, minting precious time for myself, and thoughts alike kept feeding the butterflies in my stomach. Yes, I know I didn’t care much about its features, for the bike was the cheapest and the most basic one, probably in the entire campus, that could just take me places. In short, I was fixated on the tattered box in my verandah.&lt;/p&gt;
&lt;p&gt;As the class ended that day, it was probably one of the fastest walks (actually interspersed bursts of sprints) that I had to my home. I unlocked the front door of my house, rushed towards the box picking the nearby scissors. Snip! Snip! I effortlessly tore open the already tattered box. But what do I discover here? This was the bike that I had ordered before ordering my replacement bike that was inferior to my first bike. Did you read too many bikes? Don’t worry. Let us call the first bike (that was canceled) the red bike and the second bike (the replacement) the blue bike and I had received the red bike in the package. Well, this was a confusing situation for me. To aggravate the matters, I woke up to another package the next day, waiting for me. I carefully went over the same exercise of tearing-up-the-tattered-cardboard. What do I find here? Yes, you guessed it right. It was the blue bike (yikes! I have 2 bikes!). That was the moment I was lost somewhere amidst the confusion of joy, awe, anxiety(over returning and refund process), and, anger (directed at Walmart and partly to myself).&lt;/p&gt;
&lt;p&gt;After several unsuccessful attempts to bring me to my senses, it took me three weeks to accept what had happened. I had neither received the refund for the red bike (which was anticipated in 8-10 business days) nor did I hear anything from Walmart till then. And I was the unexpected owner of two bikes and I was presented with the choice of red cycle versus the blue cycle. Although it sounds like the iconic red and blue pills of Keanu Reaves’ Matrix. But making a decision here was not that hard. I decided to keep the red one but what to do with the blue bike? Luckily for me, one of my housemates did not have a bike and it took me a little bit of convincing to sell my bike to him (how I marketed my product could be a topic for a separate blog post).&lt;/p&gt;
&lt;p&gt;Now, if you are wondering if this was my “happily ever after” moment, then hold your horses. After a week, almost a month had passed since the refund for my red bike was initiated. It seems like the stars aligned well that day and I got my refund. So now I had two bikes one of which was free! Finally, I was relieved. But in my hindsight, I had a sinister feeling that something was not right. Moreover, one of my friends, after hearing my case, subtly prompted that this seems illegal and I could be up for some trouble.  Firing up my researcher instincts, I scoured through the internet to verify if I am the unlucky one. Well to my surprise, not one or two but hundreds of cases like this crop up every year due to logistical goof-ups. To make the matters worse, I also read that some state jurisdictions consider this as theft and a 6th-degree felony leading up to a year’s prison term! Instantly, my relief switched to paranoia. Do I have to return my bike? My favorite red bike? Or even worse, will I be incarcerated? To remedy this situation, I dialed the Walmart service center once again and apologetically explained myself to the support staff. After a momentary pause, which seemed like eons to me, I heard the three golden words - “Just keep it!”. No questions asked. Period. By this time, I was so used to my paranoia that I breathed a sigh of relief anyway. I finally owned a freebie bike legally. How cool is that?&lt;/p&gt;
&lt;p&gt;Even now, just to give my creativity some rest, I recycle this story as icebreakers in conversations during parties, dinners, and even in classrooms! (What’s worse, I can refer to this blog from now on.) My audience generally compliments my honesty and some are even surprised that I called the support center to clarify. But little do they know that this should be credited to my guilt and paranoia. Looking back at this whole episode, I do and don’t blame Walmart for this episode. But I can’t complain either. On the flip side, I always cherish how I came so close to stealing a bike and getting away with it!&lt;/p&gt;
&lt;p&gt;If you have made it so far, I thank you for your awesomeness.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets</title>
      <link>https://srihegde.github.io/publication/iccv2021/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/iccv2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-Domain Multi-task Learning for Object Detection and Saliency Estimation</title>
      <link>https://srihegde.github.io/publication/cvprw2021/</link>
      <pubDate>Sat, 19 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/cvprw2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Empirical Study of Iterative Knowledge Distillation for Neural Network Compression</title>
      <link>https://srihegde.github.io/publication/esann2020/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/esann2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variational Student: Learning Compact and Sparser Networks in Knowledge Distillation Framework</title>
      <link>https://srihegde.github.io/publication/icassp2020/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/icassp2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SmartOverlays: A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces</title>
      <link>https://srihegde.github.io/publication/wacv2020/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/wacv2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Neural Network Compression</title>
      <link>https://srihegde.github.io/project/nncomp/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/nncomp/</guid>
      <description>&lt;p&gt;Compressing the memory-intensive DNN models with a minimal compromise in model accuracy using variational methods and knowledge distillation. Another part of projects involves proposing theoretical guarantees on the knowledge distillation models for efficient neural architecture search.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reverse VooDoo</title>
      <link>https://srihegde.github.io/arxiv_projects/animation/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/arxiv_projects/animation/</guid>
      <description>&lt;p&gt;Automating the animation through learning and transferring motion cues from the real RGB-D videos to virtual 3D meshes. We use graph based structure correspondences to map the motion between the two 3D entities such as point cloud and 3D meshes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SmartOverlays</title>
      <link>https://srihegde.github.io/project/smartov/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/smartov/</guid>
      <description>&lt;p&gt;A novel method for the placement of labels corresponding to objects of interest in images/videos/live feeds that is non-intrusive, relevant and temporally coherent. We employ different techniques ranging from search space optimization to visual saliency based neural network framework.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DrawInAir: A Lightweight Gestural Interface Based on Fingertip Regression</title>
      <link>https://srihegde.github.io/publication/eccv2018/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/eccv2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Where To Place: A Real-Time Visual Saliency Based Label Placement for Augmented Reality Applications</title>
      <link>https://srihegde.github.io/publication/icip2018/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/icip2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Draw In Air</title>
      <link>https://srihegde.github.io/project/drawinair/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/drawinair/</guid>
      <description>&lt;p&gt;Hand gesture classification through fingertip coordinate regression in a temporal model for touch-less interactions in AR. We highlight how a model, that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better over end-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust 3D Reconstruction of Indoor Scenes using Deep Learning</title>
      <link>https://srihegde.github.io/project/btp/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/btp/</guid>
      <description>&lt;p&gt;Employing CNNs for an end-to-end reconstruction of the indoor scenes through camera relocalization, through PoseNet, and depth estimation, through multi-scale fully convolutional network, from a single RGB image during inference and registering the 3D reconstructed patches through iterative closest point algorithm. A portion of the dataset collected during the project is also released.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GestAR: Real Time Gesture Interaction for AR with Egocentric View</title>
      <link>https://srihegde.github.io/publication/ismar2016/</link>
      <pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/publication/ismar2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distributed Fault Tolerant Multi-Robot Area Coverage Under Limited Communication Ranges</title>
      <link>https://srihegde.github.io/project/mas/</link>
      <pubDate>Sun, 01 May 2016 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/mas/</guid>
      <description>&lt;p&gt;This work develops a distributed fault tolerant area coverage algorithm, resulting in quick detection of the faulty agent under limited communication constraints and redistributes the area without conflicts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modelling Vegetation with L-Systems Using an Image</title>
      <link>https://srihegde.github.io/project/lsys/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/lsys/</guid>
      <description>&lt;p&gt;Realistic models of vegetations are very essential piece of immersive virtual environment simulations. We develop a novel technique to convert a single captured image of a vegetation to a 3D model using L-Systems, a context-free grammar that we adapt to procedurally model vegetation. We also propose a pipeline that is semi-automated with manual interventions for accurate identification of tree branches and trunk.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vision Based Outdoor Localization</title>
      <link>https://srihegde.github.io/arxiv_projects/visloc/</link>
      <pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/arxiv_projects/visloc/</guid>
      <description>&lt;p&gt;Estimating GPS location of a single RGB image of outdoor environment by, firstly, GPS coordinate retrieval from image classification and secondly, fine tuning the location estimate using structure from motion and and position triangulation. This application was interfaced by an Android mobile application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sketch23D</title>
      <link>https://srihegde.github.io/project/sketch23d/</link>
      <pubDate>Tue, 01 Dec 2015 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/sketch23d/</guid>
      <description>&lt;p&gt;We present an interactive sketching interface for quick and easy designing of freeform 3D models using OpenGL and CGAL libraries in C++. The mesh construction employs Shewchuk&amp;rsquo;s algorithm (which is based on Delaunay Triangulation). The freehand interaction and mesh construction is perfomed in real-time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Virtual Campus Project</title>
      <link>https://srihegde.github.io/project/vcp/</link>
      <pubDate>Tue, 01 Dec 2015 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/vcp/</guid>
      <description>&lt;p&gt;This project proposes to build a virtual smart campus infrastructure. A 3D interactive and immersive virtual/mixed reality environment will be designed to support geospatial services including smart navigation and telepresence. Two key aspects of the system are 3D modeling and rendering.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating Cloud of Local Servers</title>
      <link>https://srihegde.github.io/arxiv_projects/vcloud/vcloud/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/arxiv_projects/vcloud/vcloud/</guid>
      <description>&lt;p&gt;Developed a web application, that takes input from the user about his/her preferences about the specification of the machine which include - operating system, main memory space, disk(storage) space, number of cores. Then we return the IP of the machine (Virtual Machine) assigned according to the mentioned preferences, for a particular amount of time. We use KVM for VM management.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Events Archive</title>
      <link>https://srihegde.github.io/news/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/news/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;17th July 2023&lt;/strong&gt;: Our work &amp;ldquo;Diffusion Models Beat GANs on Image Classification&amp;rdquo; published on Arxiv. Check it out &lt;a href=&#34;https://arxiv.org/pdf/2307.08702.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;21st May 2023&lt;/strong&gt;: Graduated from UMD with masters in Computer Science!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;18th Oct 2021&lt;/strong&gt;: My co-authored work &amp;ldquo;Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets&amp;rdquo;  won the best paper running up award at ICCV (HTCV &amp;lsquo;21)! Congrats to all the co-authors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;1st Aug 2021&lt;/strong&gt;: My co-authored work &amp;ldquo;Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets&amp;rdquo; accepted at ICCV (HTCV &amp;lsquo;21) as a oral paper. My first publication as a UMD student!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;25th May 2021&lt;/strong&gt;: I have started my grad school at University of Maryland, College Park.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;13th Apr 2021&lt;/strong&gt;: My co-authored work &amp;ldquo;Cross-Domain Multi-task Learning for Object Detection and Saliency Estimation&amp;rdquo; accepted at CVPR (CLVISION &amp;lsquo;21) as a poster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;14th Feb 2021&lt;/strong&gt;: Received TCS Citation Award for the second time for outstanding contributions to the organization through publications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;20th July 2020&lt;/strong&gt;: Invited as a reviewer for IEEE Transactions on Circuits and Systems for Video Technology (TCSVT, impact factor: 3.599). My first journal reviewing experience!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;21st Mar 2020&lt;/strong&gt;: TreasAR Hunt - an AR based treasure hunt - organized for Re.Fresh 2020 at TCS Innovation Labs New Delhi. The &lt;a href=&#34;https://github.com/srihegde/TreasAR-Hunt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source code&lt;/a&gt; is now available. Check it out!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;24th Jan 2020&lt;/strong&gt;: Variational Student: Our work on neural network compression through sparsification in Knowledge Distillation framework got accepted as an &lt;strong&gt;&amp;ldquo;oral presentation&amp;rdquo;&lt;/strong&gt; at &lt;a href=&#34;https://2020.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2020&lt;/a&gt; to be held at Barcelona, Spain.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;24th Jan 2020&lt;/strong&gt;: IKD: Our work on empirical analysis of iterative knowledge distillation methods got accepted at &lt;a href=&#34;https://www.esann.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESANN 2020&lt;/a&gt; to be held at Bruges, Belgium.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;17th Sep 2019&lt;/strong&gt;: SmartOverlays: Our work on situated visualization in AR and video application got accepted at &lt;a href=&#34;http://wacv20.wacv.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WACV 2020&lt;/a&gt; to be held at Aspen, Colorado.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;6th Jul 2019&lt;/strong&gt;: Attended EEML Summer School 2019, Bucharest, Romania. My experience was &lt;a href=&#34;https://www.linkedin.com/posts/srihegde_eeml2019-tcsresearch-bayesianlearning-activity-6553721146418790400-5fV8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;awesome&lt;/a&gt;!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;15th March 2019&lt;/strong&gt;: Selected for &lt;a href=&#34;https://www.eeml.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EEML Summer School 2019&lt;/a&gt;. Travelling to Bucharest, Romania in July.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;21st Jan 2019&lt;/strong&gt;: TCS-PanIIT Conclave 2019- &lt;a href=&#34;https://www.linkedin.com/posts/srihegde_hackathon-iit-paniit-activity-6495213414233796608-GlZh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hackathon Update&lt;/a&gt;: Mentored teams finished at 3rd and 4th positions in the event. Kudos to the team members!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;14th Jan 2019&lt;/strong&gt;: Selected as mentor for PanIIT Hackathon at &lt;a href=&#34;https://www.tcs.com/paniit-conclave-2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TCS-PanIIT Conclave 2019&lt;/a&gt; organized by TCS and IIT Delhi.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Network Traffic Generator and Packet Analyzer</title>
      <link>https://srihegde.github.io/arxiv_projects/netpack/netpack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/arxiv_projects/netpack/netpack/</guid>
      <description>&lt;p&gt;In this project we developed an IPv4 packet generator to simulate any network traffic the user wants. We interfaced this through a web application to provide user the provision for customizing source and destination IPs and ports,the protocol to be used and amount of that data each of the packets carry. We analyzed the most used protocols and identifying clogging points in network.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
