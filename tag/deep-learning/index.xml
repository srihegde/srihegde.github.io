<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning | Srinidhi Hegde</title>
    <link>https://srihegde.github.io/tag/deep-learning/</link>
      <atom:link href="https://srihegde.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>deep learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Some rights reserved ©2024. Srinidhi Hegde</copyright><lastBuildDate>Sat, 01 Jun 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://srihegde.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>deep learning</title>
      <link>https://srihegde.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>NARViS</title>
      <link>https://srihegde.github.io/project/narvis/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/narvis/</guid>
      <description>&lt;p&gt;Exploring scientific datasets with billions of samples in real-time visualization presents a challenge - balancing high-fidelity rendering with speed. This work introduces a novel method that uses the neural deferred rendering framework to visualize large-scale scientific point cloud data. Our approach augments a real-time high-quality point-based rendering pipeline with neural post-processing, enabling real-time manipulation of the scene elements. The modularity of the proposed framework allows customization of different stages of the rendering pipeline making it applicable to various post-processing effects. Furthermore, we also show that we can achieve a high-quality visualization with the desired post-processing effects without using the full resolution of the original point cloud. Our method prioritizes speed and scalability, making it ideal for interactive exploration and analysis of large-scale scientific datasets. Furthermore, we showcase our approach’s effectiveness on various complex scientific datasets, demonstrating real-time rendering with user-defined styles and improved visual clarity, ultimately fostering a more efficient and engaging scientific visualization experience.&lt;/p&gt;
&lt;p&gt;Pre-print and code to be released soon. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Neural Network Compression</title>
      <link>https://srihegde.github.io/project/nncomp/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/nncomp/</guid>
      <description>&lt;p&gt;Compressing the memory-intensive DNN models with a minimal compromise in model accuracy using variational methods and knowledge distillation. Another part of projects involves proposing theoretical guarantees on the knowledge distillation models for efficient neural architecture search.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Draw In Air</title>
      <link>https://srihegde.github.io/project/drawinair/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/drawinair/</guid>
      <description>&lt;p&gt;Hand gesture classification through fingertip coordinate regression in a temporal model for touch-less interactions in AR. We highlight how a model, that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better over end-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust 3D Reconstruction of Indoor Scenes using Deep Learning</title>
      <link>https://srihegde.github.io/project/btp/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      <guid>https://srihegde.github.io/project/btp/</guid>
      <description>&lt;p&gt;Employing CNNs for an end-to-end reconstruction of the indoor scenes through camera relocalization, through PoseNet, and depth estimation, through multi-scale fully convolutional network, from a single RGB image during inference and registering the 3D reconstructed patches through iterative closest point algorithm. A portion of the dataset collected during the project is also released.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
